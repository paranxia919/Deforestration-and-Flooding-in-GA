{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_31525/1000392176.py:10: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_storm_data():\n",
    "    # Replace 'Storm Data 2000' with the exact folder name you see in VSCode\n",
    "    folder_path = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"  \n",
    "    pattern = os.path.join(folder_path, \"*.csv\")  # This looks for any CSV in that folder\n",
    "    \n",
    "    csv_files = glob.glob(pattern)  # Returns a list of all matching CSV file paths\n",
    "    \n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    if dataframes:\n",
    "        # Combine all individual CSV DataFrames into one big DataFrame\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        # If no files are found or the folder is empty, return an empty DataFrame\n",
    "        return pd.DataFrame()\n",
    "\n",
    "storm_df = load_storm_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rename_storm_files(folder_path):\n",
    "    \"\"\"\n",
    "    Renames files in 'folder_path' from:\n",
    "      StormEvents_details-ftp_v1.0_d2000_c20220425.csv\n",
    "    to:\n",
    "      StormDetail_2000.csv\n",
    "    \n",
    "    Assumes each filename contains '_dXXXX_' where XXXX is the year.\n",
    "    \"\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Only consider CSVs that match the general pattern\n",
    "        if file_name.startswith(\"StormEvents_details-ftp_v1.0_d\") and file_name.endswith(\".csv\"):\n",
    "            # Extract the 4-digit year from something like '_d2000_'\n",
    "            match = re.search(r\"_d(\\d{4})_\", file_name)\n",
    "            if match:\n",
    "                year = match.group(1)  # e.g. '2000'\n",
    "                # Build your new file name\n",
    "                new_name = f\"StormDetail_{year}.csv\"\n",
    "                \n",
    "                old_path = os.path.join(folder_path, file_name)\n",
    "                new_path = os.path.join(folder_path, new_name)\n",
    "                \n",
    "                # Rename the file\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"Renamed '{file_name}' -> '{new_name}'\")\n",
    "\n",
    "rename_storm_files(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "['StormDetail_2010.csv', 'StormDetail_2004.csv', 'StormDetail_2005.csv', 'StormDetail_2011.csv', 'StormDetail_2007.csv', 'StormDetail_2013.csv', 'StormDetail_2012.csv', 'StormDetail_2006.csv', '.DS_Store', 'StormDetail_2002.csv', 'StormDetail_2016.csv', 'StormDetail_2017.csv', 'StormDetail_2003.csv', 'StormDetail_2015.csv', 'StormDetail_2001.csv', 'StormDetail_2000.csv', 'StormDetail_2014.csv', 'StormDetail_2019.csv', 'StormDetail_2024.csv', 'StormDetail_2018.csv', 'StormDetail_2023.csv', 'StormDetail_2022.csv', 'StormDetail_2020.csv', 'StormDetail_2008.csv', 'StormDetail_2009.csv', 'StormDetail_2021.csv']\n",
      "After:\n",
      "['StormDetail_2010.csv', 'StormDetail_2004.csv', 'StormDetail_2005.csv', 'StormDetail_2011.csv', 'StormDetail_2007.csv', 'StormDetail_2013.csv', 'StormDetail_2012.csv', 'StormDetail_2006.csv', '.DS_Store', 'StormDetail_2002.csv', 'StormDetail_2016.csv', 'StormDetail_2017.csv', 'StormDetail_2003.csv', 'StormDetail_2015.csv', 'StormDetail_2001.csv', 'StormDetail_2000.csv', 'StormDetail_2014.csv', 'StormDetail_2019.csv', 'StormDetail_2024.csv', 'StormDetail_2018.csv', 'StormDetail_2023.csv', 'StormDetail_2022.csv', 'StormDetail_2020.csv', 'StormDetail_2008.csv', 'StormDetail_2009.csv', 'StormDetail_2021.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\")\n",
    "print(os.listdir(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"))\n",
    "\n",
    "rename_storm_files(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\")\n",
    "\n",
    "print(\"After:\")\n",
    "print(os.listdir(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find year in 'Location_2011.csv' (no rename done).\n",
      "Could not find year in 'Location_2005.csv' (no rename done).\n",
      "Could not find year in 'Location_2004.csv' (no rename done).\n",
      "Could not find year in 'Location_2010.csv' (no rename done).\n",
      "Could not find year in 'Location_2006.csv' (no rename done).\n",
      "Could not find year in 'Location_2012.csv' (no rename done).\n",
      "Could not find year in 'Location_2017.csv' (no rename done).\n",
      "Could not find year in 'Location_2002.csv' (no rename done).\n",
      "Could not find year in 'Location_2014.csv' (no rename done).\n",
      "Could not find year in 'Location_2015.csv' (no rename done).\n",
      "Could not find year in 'Location_2024.csv' (no rename done).\n",
      "Could not find year in 'Location_2018.csv' (no rename done).\n",
      "Could not find year in 'Location_2022.csv' (no rename done).\n",
      "Could not find year in 'Location_2023.csv' (no rename done).\n",
      "Could not find year in 'Location_2021.csv' (no rename done).\n",
      "Could not find year in 'Location_2008.csv' (no rename done).\n",
      "   YEARMONTH  EPISODE_ID  EVENT_ID  LOCATION_INDEX  RANGE AZIMUTH   LOCATION  \\\n",
      "0     201104       48862    285976               1   0.57       E    GILLHAM   \n",
      "1     201104       48862    285978               1   2.39     ESE  CENTER PT   \n",
      "2     201104       48862    285979               1   0.00       N   DE QUEEN   \n",
      "3     201104       48862    285980               1   0.90      NW   DE QUEEN   \n",
      "4     201104       48862    285981               1   0.57       E       DIAN   \n",
      "\n",
      "   LATITUDE  LONGITUDE       LAT2       LON2  \n",
      "0     34.17     -94.31  3410200.0  9418600.0  \n",
      "1     34.02     -93.89   341200.0  9353400.0  \n",
      "2     34.03     -94.33   341800.0  9419800.0  \n",
      "3     34.04     -94.34   342400.0  9420400.0  \n",
      "4     33.80     -93.39  3348000.0  9323400.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_location_data(folder_path):\n",
    "    \"\"\"\n",
    "    1) Loads all 'StormEvents_locations-ftp_v1.0_dXXXX_*.csv' files from 'folder_path'.\n",
    "    2) Concatenates them into a single DataFrame.\n",
    "    3) Renames each file from 'StormEvents_locations-ftp_v1.0_dYYYY_c...csv'\n",
    "       to 'Location_YYYY.csv'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the pattern to find files\n",
    "    pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    # Load them into a list of DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        location = pd.read_csv(file_path)\n",
    "        dataframes.append(location)\n",
    "\n",
    "    # Rename each file based on the 4-digit year in the pattern 'dYYYY'\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        match = re.search(r\"_d(\\d{4})_\", file_name)\n",
    "        if match:\n",
    "            year = match.group(1)  # e.g., '2018'\n",
    "            new_file_name = f\"Location_{year}.csv\"\n",
    "            new_file_path = os.path.join(folder_path, new_file_name)\n",
    "            \n",
    "            # Rename on disk\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f\"Renamed '{file_name}' -> '{new_file_name}'\")\n",
    "        else:\n",
    "            print(f\"Could not find year in '{file_name}' (no rename done).\")\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    if dataframes:\n",
    "        combined_location = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_location\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024\"\n",
    "location_df = load_location_data(folder)\n",
    "print(location_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EVENT_ID   LOCATION\n",
      "0    285976    GILLHAM\n",
      "1    285978  CENTER PT\n",
      "2    285979   DE QUEEN\n",
      "3    285980   DE QUEEN\n",
      "4    285981       DIAN\n"
     ]
    }
   ],
   "source": [
    "def select_event_location_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame from your location files,\n",
    "    this function returns a new DataFrame containing only the\n",
    "    'EVENT_ID' and 'LOCATION' columns.\n",
    "    \n",
    "    If the columns are named differently (e.g., 'Location' instead of 'LOCATION'),\n",
    "    update the column names accordingly.\n",
    "    \"\"\"\n",
    "    # Select only the desired columns.\n",
    "    # Adjust the names if your actual DataFrame has a different case/spelling.\n",
    "    selected_df = df[['EVENT_ID', 'LOCATION']]\n",
    "    \n",
    "    return selected_df\n",
    "\n",
    "\n",
    "selected_location_df = select_event_location_columns(location_df)\n",
    "print(selected_location_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location file for year 2007 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2013 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_31525/1149701510.py:29: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  storm_df = pd.read_csv(storm_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location file for year 2016 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2003 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2001 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2000 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2019 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2020 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2009 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "     STATE        LOCATION   EVENT_TYPE  EVENT_ID\n",
      "0  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "1  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "2  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "3  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "4     IOWA    NEW HARTFORD        Flood    214762\n"
     ]
    }
   ],
   "source": [
    "def merge_storm_detail_location(storm_detail_folder, location_folder):\n",
    "    \"\"\"\n",
    "    For each StormDetail_XXXX.csv file in storm_detail_folder, this function:\n",
    "      1. Extracts the year (XXXX) from the filename.\n",
    "      2. Looks for a corresponding Location_XXXX.csv file in location_folder.\n",
    "      3. Loads both CSVs.\n",
    "      4. Merges them on 'EVENT_ID'.\n",
    "      5. Selects only the 'EVENT_ID' and 'location' columns.\n",
    "    \n",
    "    It returns a combined DataFrame with merged data for all years.\n",
    "    \"\"\"\n",
    "    # Pattern for storm detail files: e.g., StormDetail_2000.csv, StormDetail_2001.csv, etc.\n",
    "    storm_pattern = os.path.join(storm_detail_folder, \"StormDetail_*.csv\")\n",
    "    storm_files = glob.glob(storm_pattern)\n",
    "    \n",
    "    merged_list = []\n",
    "    \n",
    "    for storm_file in storm_files:\n",
    "        # Extract the 4-digit year from the filename\n",
    "        base_name = os.path.basename(storm_file)\n",
    "        match = re.search(r\"StormDetail_(\\d{4})\\.csv\", base_name)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            # Construct the expected location file name for that year.\n",
    "            location_file = os.path.join(location_folder, f\"Location_{year}.csv\")\n",
    "            \n",
    "            if os.path.exists(location_file):\n",
    "                # Load the two dataframes.\n",
    "                storm_df = pd.read_csv(storm_file)\n",
    "                location_df = pd.read_csv(location_file)\n",
    "                \n",
    "                # Merge them on EVENT_ID.\n",
    "                merged_df = pd.merge(storm_df, location_df, on='EVENT_ID', how='inner')\n",
    "                \n",
    "                # Select only the EVENT_ID and location columns.\n",
    "                # (Assumes location_df already has the 'location' column; if not, \n",
    "                # you may need to rename the appropriate column first.)\n",
    "                merged_df = merged_df[['STATE', 'LOCATION', 'EVENT_TYPE', 'EVENT_ID']]\n",
    "                \n",
    "                merged_list.append(merged_df)\n",
    "            else:\n",
    "                print(f\"Location file for year {year} not found in {location_folder}.\")\n",
    "        else:\n",
    "            print(f\"Year not found in file name: {base_name}\")\n",
    "    \n",
    "    if merged_list:\n",
    "        # Combine all merged DataFrames into one.\n",
    "        final_merged = pd.concat(merged_list, ignore_index=True)\n",
    "        return final_merged\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "storm_detail_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"\n",
    "location_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024\"\n",
    "\n",
    "merged_data = merge_storm_detail_location(storm_detail_folder, location_folder)\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATE     LOCATION   EVENT_TYPE  EVENT_ID\n",
      "6152  GEORGIA  CENTRAL JCT  Flash Flood    246410\n",
      "6153  GEORGIA  CENTRAL JCT  Flash Flood    246410\n",
      "6154  GEORGIA     SAVANNAH  Flash Flood    246410\n",
      "6155  GEORGIA     SAVANNAH  Flash Flood    246410\n",
      "8623  GEORGIA      CLAXTON  Flash Flood    235603\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE CELL\n",
    "\n",
    "def filter_storm_data(merged_data):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame so that:\n",
    "      - Only rows where STATE == 'GEORGIA' \n",
    "      - Only rows where EVENT_TYPE is in ['flash flood', 'coastal flood', 'flood']\n",
    "      - Keeps only the columns ['STATE', 'LOCATION_ID']\n",
    "    \"\"\"\n",
    "    # Filter rows\n",
    "    filtered_df = merged_data[\n",
    "        (merged_data[\"STATE\"] == \"GEORGIA\") &\n",
    "        ((merged_data['EVENT_TYPE'] == 'Flash Flood') | \n",
    "              (merged_data['EVENT_TYPE'] == 'Coastal Flood') |\n",
    "              (merged_data['EVENT_TYPE'] == 'Flood') )\n",
    "    ]\n",
    "    \n",
    "    # Select only the desired columns\n",
    "    filtered_df = filtered_df[[\"STATE\", \"LOCATION\", \"EVENT_TYPE\", \"EVENT_ID\"]]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "filtered_storm_df = filter_storm_data(merged_data)\n",
    "print(filtered_storm_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       LOCATION  Event_Count\n",
      "0          (AGS)BUSH FLD AUGUST            1\n",
      "1           (CSG)COLUMBUS METRO            2\n",
      "2          (GVL)GAINESVILLE MEM            2\n",
      "3          (LSF)LAWSON AAF FT B            1\n",
      "4  (MGR)MOULTRIE MUNICIPAL ARPT            5\n"
     ]
    }
   ],
   "source": [
    "def count_event_ids_by_location(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with 'LOCATION' and 'EVENT_ID' columns,\n",
    "    groups by LOCATION and counts the number of EVENT_ID occurrences.\n",
    "    \n",
    "    Returns a new DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'Event_Count'\n",
    "    \"\"\"\n",
    "    # Group by 'LOCATION' and count the occurrences of 'EVENT_ID'\n",
    "    count_df = df.groupby('LOCATION')['EVENT_ID'].count().reset_index()\n",
    "    count_df.rename(columns={'EVENT_ID': 'Event_Count'}, inplace=True)\n",
    "    return count_df\n",
    "\n",
    "\n",
    "event_counts = count_event_ids_by_location(filtered_storm_df)\n",
    "print(event_counts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_rate_change(filtered_df, year1=1985, year2=2023):\n",
    "    \"\"\"\n",
    "    Given a DataFrame that includes a 'Year' column along with \n",
    "    'LOCATION' and 'EVENT_ID' columns, this function:\n",
    "      1. Filters the data to only include rows for year1 and year2.\n",
    "      2. Counts the number of events (EVENT_ID) by LOCATION for each year.\n",
    "      3. Merges the counts (inner join on LOCATION, so only locations present in both years are kept).\n",
    "      4. Calculates the rate of change using:\n",
    "         (count_year2 - count_year1) / count_year1.\n",
    "    \n",
    "    Returns a DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'count_year1'\n",
    "      - 'count_year2'\n",
    "      - 'rate_change'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for each target year\n",
    "    df_year1 = filtered_df[filtered_df['Year'] == year1]\n",
    "    df_year2 = filtered_df[filtered_df['Year'] == year2]\n",
    "    \n",
    "    # Count events per location for each year\n",
    "    count_year1 = df_year1.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year1.rename(columns={'EVENT_ID': 'count_year1'}, inplace=True)\n",
    "    \n",
    "    count_year2 = df_year2.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year2.rename(columns={'EVENT_ID': 'count_year2'}, inplace=True)\n",
    "    \n",
    "    # Merge counts for locations present in both years\n",
    "    merged_counts = pd.merge(count_year1, count_year2, on='LOCATION', how='inner')\n",
    "    \n",
    "    # Calculate the rate of change. If count_year1 is zero, the division will produce inf/NaN.\n",
    "    merged_counts['rate_change'] = (merged_counts['count_year2'] - merged_counts['count_year1']) / merged_counts['count_year1']\n",
    "    \n",
    "    return merged_counts\n",
    "\n",
    "\n",
    "rate_change_df = calculate_rate_change(filtered_storm_df, year1=1985, year2=2023)\n",
    "print(rate_change_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_change_df.to_csv(\"rate_change_per_location.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
