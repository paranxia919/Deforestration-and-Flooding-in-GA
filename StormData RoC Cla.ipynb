{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_36680/1665983800.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_36680/1470834919.py:10: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_36680/1470834919.py:10: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_36680/1470834919.py:10: DtypeWarning: Columns (26,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_36680/1470834919.py:10: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_36680/1470834919.py:10: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_storm_data():\n",
    "    folder_path = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"  \n",
    "    pattern = os.path.join(folder_path, \"StormDetail_*.csv\")  \n",
    "    \n",
    "    csv_files = glob.glob(pattern)  # Returns a list of all matching CSV file paths\n",
    "    \n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "storm_df = load_storm_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BEGIN_YEARMONTH",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BEGIN_DAY",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BEGIN_TIME",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "END_YEARMONTH",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "END_DAY",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "END_TIME",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EPISODE_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EVENT_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "STATE_FIPS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "YEAR",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MONTH_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EVENT_TYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CZ_TYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CZ_FIPS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "WFO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "BEGIN_DATE_TIME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CZ_TIMEZONE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "END_DATE_TIME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "INJURIES_DIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "INJURIES_INDIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DEATHS_DIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DEATHS_INDIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DAMAGE_PROPERTY",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DAMAGE_CROPS",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SOURCE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MAGNITUDE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAGNITUDE_TYPE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "FLOOD_CAUSE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "CATEGORY",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_F_SCALE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TOR_LENGTH",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_WIDTH",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_OTHER_WFO",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TOR_OTHER_CZ_STATE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TOR_OTHER_CZ_FIPS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_OTHER_CZ_NAME",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "BEGIN_RANGE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BEGIN_AZIMUTH",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "BEGIN_LOCATION",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "END_RANGE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "END_AZIMUTH",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "END_LOCATION",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "BEGIN_LAT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BEGIN_LON",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "END_LAT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "END_LON",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "EPISODE_NARRATIVE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EVENT_NARRATIVE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "DATA_SOURCE",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "eeba04ce-faa3-4cf7-9e93-e41e8d181676",
       "rows": [
        [
         "711497",
         "202407",
         "4",
         "1500",
         "202407",
         "4",
         "1600",
         "195150",
         "1206283",
         "ALABAMA",
         "1.0",
         "2024",
         "July",
         "Excessive Heat",
         "Z",
         "16",
         "CULLMAN",
         "HUN",
         "04-JUL-24 15:00:00",
         "CST-6",
         "04-JUL-24 16:00:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "AWOS",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Heat index values reached 105 to 109 degrees for much of the late morning and afternoon hours in north central and northwest Alabama. The heat index peaked at 110 to 112 degrees for a few hours during the mid afternoon.",
         "The heat index peaked at 110 degrees for a short period.",
         "CSV"
        ],
        [
         "711498",
         "202407",
         "4",
         "1100",
         "202407",
         "4",
         "1800",
         "195150",
         "1206284",
         "ALABAMA",
         "1.0",
         "2024",
         "July",
         "Heat",
         "Z",
         "16",
         "CULLMAN",
         "HUN",
         "04-JUL-24 11:00:00",
         "CST-6",
         "04-JUL-24 18:00:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "AWOS",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Heat index values reached 105 to 109 degrees for much of the late morning and afternoon hours in north central and northwest Alabama. The heat index peaked at 110 to 112 degrees for a few hours during the mid afternoon.",
         "The heat index reached 105 to 109 degrees.",
         "CSV"
        ],
        [
         "711499",
         "202407",
         "6",
         "1258",
         "202407",
         "6",
         "1258",
         "192857",
         "1198392",
         "NEBRASKA",
         "31.0",
         "2024",
         "July",
         "Hail",
         "C",
         "47",
         "DAWSON",
         "GID",
         "06-JUL-24 12:58:00",
         "CST-6",
         "06-JUL-24 12:58:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "Public",
         "1.0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "8.0",
         "SW",
         "LEXINGTON ARPT",
         "8.0",
         "SW",
         "LEXINGTON ARPT",
         "40.71",
         "-99.88",
         "40.71",
         "-99.88",
         "A broad, upper trough was over most of the country, including Nebraska. An upper disturbance passed over the central Plains which created enhanced lift.  CAPE values of around 2,000 J/kg, wind shear of around 40 to 50 knots, and mid-level lapse rates of 7 to 8 degrees C/km were over south-central Nebraska. Temperatures were mostly in the 70s to lower 80s. These conditions resulted in an atmosphere that was conducive for severe storm development.||Thunderstorms had already developed to the northwest of the area during the early to mid-morning hours and moved over central Nebraska during the late morning hours. Additional storms developed to the west and northwest of the area during the late morning hours. These storms moved eastward and one storm began impacting Dawson County with hail up to the size of tennis balls just after 12 PM CDT. This storm continued moving east southeast through the afternoon with other storms moving in behind the original storm. The largest hail of the day was baseball sized and fell in Phelps County after 2 PM CDT. Additional hail impacted Dawson, Gosper, Buffalo, Phelps, Kearney, Franklin, Webster, and Fillmore Counties. These storms produced 5 tornadoes which impacted Kearney, Adams, Clay, and Fillmore Counties. The first two tornadoes impacted Kearney County and were both rated EF1. The third and fourth tornadoes impacted Adams County and were both rated EF0. The fifth tornado first touched down in Clay County then moved into Fillmore County and was rated an EF1. Severe wind gusts were also reported with these storms with the strongest winds estimated near 80 mph in Kearney and Adams Counties. The storm reports ended just before 6 PM CDT as the storms moved east of the area.",
         null,
         "CSV"
        ],
        [
         "711500",
         "202407",
         "6",
         "1523",
         "202407",
         "6",
         "1535",
         "192857",
         "1198427",
         "NEBRASKA",
         "31.0",
         "2024",
         "July",
         "Thunderstorm Wind",
         "C",
         "1",
         "ADAMS",
         "GID",
         "06-JUL-24 15:23:00",
         "CST-6",
         "06-JUL-24 15:35:00",
         "0",
         "0",
         "0",
         "0",
         "15.00K",
         "0.00K",
         "ASOS",
         "60.0",
         "MG",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "N",
         "(HSI)HASTINGS ARPT",
         "1.0",
         "SE",
         "HASTINGS",
         "40.6",
         "-98.43",
         "40.57",
         "-98.3696",
         "A broad, upper trough was over most of the country, including Nebraska. An upper disturbance passed over the central Plains which created enhanced lift.  CAPE values of around 2,000 J/kg, wind shear of around 40 to 50 knots, and mid-level lapse rates of 7 to 8 degrees C/km were over south-central Nebraska. Temperatures were mostly in the 70s to lower 80s. These conditions resulted in an atmosphere that was conducive for severe storm development.||Thunderstorms had already developed to the northwest of the area during the early to mid-morning hours and moved over central Nebraska during the late morning hours. Additional storms developed to the west and northwest of the area during the late morning hours. These storms moved eastward and one storm began impacting Dawson County with hail up to the size of tennis balls just after 12 PM CDT. This storm continued moving east southeast through the afternoon with other storms moving in behind the original storm. The largest hail of the day was baseball sized and fell in Phelps County after 2 PM CDT. Additional hail impacted Dawson, Gosper, Buffalo, Phelps, Kearney, Franklin, Webster, and Fillmore Counties. These storms produced 5 tornadoes which impacted Kearney, Adams, Clay, and Fillmore Counties. The first two tornadoes impacted Kearney County and were both rated EF1. The third and fourth tornadoes impacted Adams County and were both rated EF0. The fifth tornado first touched down in Clay County then moved into Fillmore County and was rated an EF1. Severe wind gusts were also reported with these storms with the strongest winds estimated near 80 mph in Kearney and Adams Counties. The storm reports ended just before 6 PM CDT as the storms moved east of the area.",
         "A wind gust of 69 MPH was measured by the Hastings Airport ASOS. Generally minor tree damage was reported across Hastings, and power outages were reported in the south part of town.",
         "CSV"
        ],
        [
         "711501",
         "202407",
         "6",
         "1645",
         "202407",
         "6",
         "1645",
         "192857",
         "1198773",
         "NEBRASKA",
         "31.0",
         "2024",
         "July",
         "Hail",
         "C",
         "59",
         "FILLMORE",
         "GID",
         "06-JUL-24 16:45:00",
         "CST-6",
         "06-JUL-24 16:45:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "Public",
         "1.5",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "1.0",
         "NW",
         "GENEVA",
         "1.0",
         "NW",
         "GENEVA",
         "40.54",
         "-97.59",
         "40.54",
         "-97.59",
         "A broad, upper trough was over most of the country, including Nebraska. An upper disturbance passed over the central Plains which created enhanced lift.  CAPE values of around 2,000 J/kg, wind shear of around 40 to 50 knots, and mid-level lapse rates of 7 to 8 degrees C/km were over south-central Nebraska. Temperatures were mostly in the 70s to lower 80s. These conditions resulted in an atmosphere that was conducive for severe storm development.||Thunderstorms had already developed to the northwest of the area during the early to mid-morning hours and moved over central Nebraska during the late morning hours. Additional storms developed to the west and northwest of the area during the late morning hours. These storms moved eastward and one storm began impacting Dawson County with hail up to the size of tennis balls just after 12 PM CDT. This storm continued moving east southeast through the afternoon with other storms moving in behind the original storm. The largest hail of the day was baseball sized and fell in Phelps County after 2 PM CDT. Additional hail impacted Dawson, Gosper, Buffalo, Phelps, Kearney, Franklin, Webster, and Fillmore Counties. These storms produced 5 tornadoes which impacted Kearney, Adams, Clay, and Fillmore Counties. The first two tornadoes impacted Kearney County and were both rated EF1. The third and fourth tornadoes impacted Adams County and were both rated EF0. The fifth tornado first touched down in Clay County then moved into Fillmore County and was rated an EF1. Severe wind gusts were also reported with these storms with the strongest winds estimated near 80 mph in Kearney and Adams Counties. The storm reports ended just before 6 PM CDT as the storms moved east of the area.",
         null,
         "CSV"
        ]
       ],
       "shape": {
        "columns": 51,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>711497</th>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1600</td>\n",
       "      <td>195150</td>\n",
       "      <td>1206283</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heat index values reached 105 to 109 degrees f...</td>\n",
       "      <td>The heat index peaked at 110 degrees for a sho...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711498</th>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1100</td>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1800</td>\n",
       "      <td>195150</td>\n",
       "      <td>1206284</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heat index values reached 105 to 109 degrees f...</td>\n",
       "      <td>The heat index reached 105 to 109 degrees.</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711499</th>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1258</td>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1258</td>\n",
       "      <td>192857</td>\n",
       "      <td>1198392</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>LEXINGTON ARPT</td>\n",
       "      <td>40.71</td>\n",
       "      <td>-99.88</td>\n",
       "      <td>40.71</td>\n",
       "      <td>-99.8800</td>\n",
       "      <td>A broad, upper trough was over most of the cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711500</th>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1523</td>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1535</td>\n",
       "      <td>192857</td>\n",
       "      <td>1198427</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>HASTINGS</td>\n",
       "      <td>40.60</td>\n",
       "      <td>-98.43</td>\n",
       "      <td>40.57</td>\n",
       "      <td>-98.3696</td>\n",
       "      <td>A broad, upper trough was over most of the cou...</td>\n",
       "      <td>A wind gust of 69 MPH was measured by the Hast...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711501</th>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1645</td>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1645</td>\n",
       "      <td>192857</td>\n",
       "      <td>1198773</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>GENEVA</td>\n",
       "      <td>40.54</td>\n",
       "      <td>-97.59</td>\n",
       "      <td>40.54</td>\n",
       "      <td>-97.5900</td>\n",
       "      <td>A broad, upper trough was over most of the cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  \\\n",
       "711497           202407          4        1500         202407        4   \n",
       "711498           202407          4        1100         202407        4   \n",
       "711499           202407          6        1258         202407        6   \n",
       "711500           202407          6        1523         202407        6   \n",
       "711501           202407          6        1645         202407        6   \n",
       "\n",
       "        END_TIME  EPISODE_ID  EVENT_ID     STATE  STATE_FIPS  ...  END_RANGE  \\\n",
       "711497      1600      195150   1206283   ALABAMA         1.0  ...        NaN   \n",
       "711498      1800      195150   1206284   ALABAMA         1.0  ...        NaN   \n",
       "711499      1258      192857   1198392  NEBRASKA        31.0  ...        8.0   \n",
       "711500      1535      192857   1198427  NEBRASKA        31.0  ...        1.0   \n",
       "711501      1645      192857   1198773  NEBRASKA        31.0  ...        1.0   \n",
       "\n",
       "       END_AZIMUTH    END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT  END_LON  \\\n",
       "711497         NaN             NaN       NaN        NaN     NaN      NaN   \n",
       "711498         NaN             NaN       NaN        NaN     NaN      NaN   \n",
       "711499          SW  LEXINGTON ARPT     40.71     -99.88   40.71 -99.8800   \n",
       "711500          SE        HASTINGS     40.60     -98.43   40.57 -98.3696   \n",
       "711501          NW          GENEVA     40.54     -97.59   40.54 -97.5900   \n",
       "\n",
       "                                        EPISODE_NARRATIVE  \\\n",
       "711497  Heat index values reached 105 to 109 degrees f...   \n",
       "711498  Heat index values reached 105 to 109 degrees f...   \n",
       "711499  A broad, upper trough was over most of the cou...   \n",
       "711500  A broad, upper trough was over most of the cou...   \n",
       "711501  A broad, upper trough was over most of the cou...   \n",
       "\n",
       "                                          EVENT_NARRATIVE DATA_SOURCE  \n",
       "711497  The heat index peaked at 110 degrees for a sho...         CSV  \n",
       "711498         The heat index reached 105 to 109 degrees.         CSV  \n",
       "711499                                                NaN         CSV  \n",
       "711500  A wind gust of 69 MPH was measured by the Hast...         CSV  \n",
       "711501                                                NaN         CSV  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_selected_storm_data(folder_path, years=[2000, 2001, 2002, 2003, 2004, 2005, 2019, 2020, 2021, 2022, 2023, 2024]):\n",
    "    dataframes = []\n",
    "    for year in years:\n",
    "        file_path = os.path.join(folder_path, f\"StormDetail_{year}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"File for year {year} does not exist.\")\n",
    "    \n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Usage\n",
    "folder_path = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"\n",
    "selected_storm_df = load_selected_storm_data(folder_path)\n",
    "\n",
    "selected_storm_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "YEAR",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EVENT_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EVENT_TYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "BEGIN_DATE_TIME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "END_DATE_TIME",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "99876699-5c17-48e8-9415-e9122798807a",
       "rows": [
        [
         "5453",
         "2000",
         "5135328",
         "BEN HILL",
         "GEORGIA",
         "Flash Flood",
         "30-MAR-00 07:00:00",
         "30-MAR-00 11:15:00"
        ],
        [
         "5731",
         "2000",
         "5135327",
         "IRWIN",
         "GEORGIA",
         "Flash Flood",
         "30-MAR-00 07:00:00",
         "30-MAR-00 11:15:00"
        ],
        [
         "5855",
         "2000",
         "5133313",
         "CRISP",
         "GEORGIA",
         "Flash Flood",
         "30-MAR-00 10:30:00",
         "30-MAR-00 10:30:00"
        ],
        [
         "5856",
         "2000",
         "5133314",
         "DOOLY",
         "GEORGIA",
         "Flash Flood",
         "30-MAR-00 10:30:00",
         "30-MAR-00 10:30:00"
        ],
        [
         "5857",
         "2000",
         "5133315",
         "WILCOX",
         "GEORGIA",
         "Flash Flood",
         "30-MAR-00 10:30:00",
         "30-MAR-00 10:30:00"
        ],
        [
         "6478",
         "2000",
         "5136974",
         "DOUGHERTY",
         "GEORGIA",
         "Flash Flood",
         "26-MAR-00 17:30:00",
         "26-MAR-00 17:30:00"
        ],
        [
         "7356",
         "2000",
         "5136734",
         "COFFEE",
         "GEORGIA",
         "Flash Flood",
         "30-MAR-00 13:00:00",
         "30-MAR-00 13:00:00"
        ],
        [
         "8521",
         "2000",
         "5138944",
         "POLK",
         "GEORGIA",
         "Flood",
         "03-APR-00 16:45:00",
         "03-APR-00 16:45:00"
        ],
        [
         "8525",
         "2000",
         "5138941",
         "FORSYTH",
         "GEORGIA",
         "Flood",
         "03-APR-00 02:30:00",
         "03-APR-00 02:30:00"
        ],
        [
         "10311",
         "2000",
         "5138932",
         "FLOYD",
         "GEORGIA",
         "Flash Flood",
         "03-APR-00 01:15:00",
         "03-APR-00 02:00:00"
        ],
        [
         "10312",
         "2000",
         "5138933",
         "BARTOW",
         "GEORGIA",
         "Flash Flood",
         "03-APR-00 01:30:00",
         "03-APR-00 02:00:00"
        ],
        [
         "11932",
         "2000",
         "5141017",
         "LOWNDES",
         "GEORGIA",
         "Flood",
         "24-APR-00 12:30:00",
         "24-APR-00 16:30:00"
        ],
        [
         "11936",
         "2000",
         "5141018",
         "BROOKS",
         "GEORGIA",
         "Flood",
         "24-APR-00 12:00:00",
         "24-APR-00 16:00:00"
        ],
        [
         "16889",
         "2000",
         "5148174",
         "RICHMOND",
         "GEORGIA",
         "Flash Flood",
         "20-JUN-00 22:14:00",
         "21-JUN-00 03:00:00"
        ],
        [
         "16890",
         "2000",
         "5148175",
         "COLUMBIA",
         "GEORGIA",
         "Flash Flood",
         "22-JUN-00 18:30:00",
         "23-JUN-00 00:00:00"
        ],
        [
         "20250",
         "2000",
         "5148110",
         "COLUMBIA",
         "GEORGIA",
         "Flash Flood",
         "20-JUN-00 20:25:00",
         "21-JUN-00 03:00:00"
        ],
        [
         "22741",
         "2000",
         "5158504",
         "BIBB",
         "GEORGIA",
         "Flood",
         "03-SEP-00 18:25:00",
         "03-SEP-00 18:25:00"
        ],
        [
         "22966",
         "2000",
         "5158141",
         "THOMAS",
         "GEORGIA",
         "Flash Flood",
         "06-SEP-00 07:45:00",
         "06-SEP-00 15:00:00"
        ],
        [
         "24746",
         "2000",
         "5158934",
         "COBB",
         "GEORGIA",
         "Flood",
         "21-SEP-00 07:00:00",
         "21-SEP-00 07:00:00"
        ],
        [
         "24815",
         "2000",
         "5158140",
         "BROOKS",
         "GEORGIA",
         "Flash Flood",
         "06-SEP-00 07:45:00",
         "06-SEP-00 15:00:00"
        ],
        [
         "25580",
         "2000",
         "5158458",
         "LOWNDES",
         "GEORGIA",
         "Flash Flood",
         "06-SEP-00 07:45:00",
         "06-SEP-00 12:00:00"
        ],
        [
         "25581",
         "2000",
         "5158459",
         "GRADY",
         "GEORGIA",
         "Flash Flood",
         "06-SEP-00 07:45:00",
         "06-SEP-00 11:00:00"
        ],
        [
         "25670",
         "2000",
         "5158443",
         "HARALSON",
         "GEORGIA",
         "Flood",
         "01-SEP-00 17:09:00",
         "01-SEP-00 17:09:00"
        ],
        [
         "25836",
         "2000",
         "5158460",
         "COLQUITT",
         "GEORGIA",
         "Flash Flood",
         "06-SEP-00 07:45:00",
         "06-SEP-00 12:00:00"
        ],
        [
         "25837",
         "2000",
         "5158461",
         "BERRIEN",
         "GEORGIA",
         "Flash Flood",
         "06-SEP-00 07:45:00",
         "06-SEP-00 12:00:00"
        ],
        [
         "25899",
         "2000",
         "5158442",
         "POLK",
         "GEORGIA",
         "Flood",
         "01-SEP-00 08:00:00",
         "01-SEP-00 08:00:00"
        ],
        [
         "26661",
         "2000",
         "5158848",
         "DE KALB",
         "GEORGIA",
         "Flash Flood",
         "21-SEP-00 07:00:00",
         "21-SEP-00 07:00:00"
        ],
        [
         "26795",
         "2000",
         "5158932",
         "WHITE",
         "GEORGIA",
         "Flood",
         "21-SEP-00 00:55:00",
         "21-SEP-00 00:55:00"
        ],
        [
         "28305",
         "2000",
         "5174279",
         "FULTON",
         "GEORGIA",
         "Flood",
         "21-SEP-00 08:05:00",
         "21-SEP-00 08:05:00"
        ],
        [
         "28306",
         "2000",
         "5174280",
         "BALDWIN",
         "GEORGIA",
         "Flood",
         "21-SEP-00 23:51:00",
         "21-SEP-00 23:51:00"
        ],
        [
         "29083",
         "2000",
         "5155188",
         "FLOYD",
         "GEORGIA",
         "Flood",
         "29-JUL-00 16:00:00",
         "29-JUL-00 16:00:00"
        ],
        [
         "32126",
         "2000",
         "5156513",
         "HALL",
         "GEORGIA",
         "Flood",
         "10-AUG-00 23:32:00",
         "10-AUG-00 23:32:00"
        ],
        [
         "33590",
         "2000",
         "5155510",
         "GWINNETT",
         "GEORGIA",
         "Flood",
         "31-JUL-00 17:50:00",
         "31-JUL-00 17:50:00"
        ],
        [
         "33659",
         "2000",
         "5155513",
         "FAYETTE",
         "GEORGIA",
         "Flood",
         "31-JUL-00 16:30:00",
         "31-JUL-00 17:10:00"
        ],
        [
         "34563",
         "2000",
         "5159822",
         "DE KALB",
         "GEORGIA",
         "Flood",
         "06-OCT-00 07:00:00",
         "06-OCT-00 07:00:00"
        ],
        [
         "35352",
         "2000",
         "5156619",
         "PAULDING",
         "GEORGIA",
         "Flood",
         "10-AUG-00 20:53:00",
         "10-AUG-00 20:53:00"
        ],
        [
         "35992",
         "2000",
         "5157227",
         "COWETA",
         "GEORGIA",
         "Flood",
         "20-AUG-00 23:00:00",
         "21-AUG-00 00:00:00"
        ],
        [
         "37277",
         "2000",
         "5156078",
         "LAURENS",
         "GEORGIA",
         "Flood",
         "12-JUL-00 13:30:00",
         "12-JUL-00 13:30:00"
        ],
        [
         "37280",
         "2000",
         "5156081",
         "EMANUEL",
         "GEORGIA",
         "Flood",
         "12-JUL-00 14:30:00",
         "12-JUL-00 14:30:00"
        ],
        [
         "38123",
         "2000",
         "5174909",
         "CAMDEN",
         "GEORGIA",
         "Flash Flood",
         "06-SEP-00 13:10:00",
         "06-SEP-00 15:00:00"
        ],
        [
         "38239",
         "2000",
         "5155507",
         "FLOYD",
         "GEORGIA",
         "Flood",
         "29-JUL-00 16:00:00",
         "29-JUL-00 16:00:00"
        ],
        [
         "38241",
         "2000",
         "5155509",
         "DE KALB",
         "GEORGIA",
         "Flood",
         "31-JUL-00 17:50:00",
         "31-JUL-00 17:50:00"
        ],
        [
         "38920",
         "2000",
         "5174351",
         "HARALSON",
         "GEORGIA",
         "Flood",
         "22-SEP-00 10:18:00",
         "22-SEP-00 10:18:00"
        ],
        [
         "46719",
         "2000",
         "5172456",
         "CHEROKEE",
         "GEORGIA",
         "Flood",
         "02-AUG-00 13:25:00",
         "02-AUG-00 13:25:00"
        ],
        [
         "46722",
         "2000",
         "5172459",
         "BIBB",
         "GEORGIA",
         "Flood",
         "04-AUG-00 15:00:00",
         "04-AUG-00 15:00:00"
        ],
        [
         "47605",
         "2000",
         "5171332",
         "FULTON",
         "GEORGIA",
         "Flood",
         "24-AUG-00 18:00:00",
         "24-AUG-00 19:00:00"
        ],
        [
         "54287",
         "2001",
         "5230841",
         "BIBB",
         "GEORGIA",
         "Flood",
         "19-JAN-01 13:21:00",
         "19-JAN-01 13:21:00"
        ],
        [
         "60552",
         "2001",
         "5238869",
         "PIKE",
         "GEORGIA",
         "Flood",
         "03-MAR-01 21:00:00",
         "03-MAR-01 23:00:00"
        ],
        [
         "64459",
         "2001",
         "5238970",
         "UPSON",
         "GEORGIA",
         "Flood",
         "12-MAR-01 17:20:00",
         "12-MAR-01 17:20:00"
        ],
        [
         "68662",
         "2001",
         "5238974",
         "PIKE",
         "GEORGIA",
         "Flood",
         "15-MAR-01 04:35:00",
         "15-MAR-01 04:35:00"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 1815
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5453</th>\n",
       "      <td>2000</td>\n",
       "      <td>5135328</td>\n",
       "      <td>BEN HILL</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>30-MAR-00 07:00:00</td>\n",
       "      <td>30-MAR-00 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5731</th>\n",
       "      <td>2000</td>\n",
       "      <td>5135327</td>\n",
       "      <td>IRWIN</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>30-MAR-00 07:00:00</td>\n",
       "      <td>30-MAR-00 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5855</th>\n",
       "      <td>2000</td>\n",
       "      <td>5133313</td>\n",
       "      <td>CRISP</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>30-MAR-00 10:30:00</td>\n",
       "      <td>30-MAR-00 10:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5856</th>\n",
       "      <td>2000</td>\n",
       "      <td>5133314</td>\n",
       "      <td>DOOLY</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>30-MAR-00 10:30:00</td>\n",
       "      <td>30-MAR-00 10:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>2000</td>\n",
       "      <td>5133315</td>\n",
       "      <td>WILCOX</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>30-MAR-00 10:30:00</td>\n",
       "      <td>30-MAR-00 10:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710912</th>\n",
       "      <td>2024</td>\n",
       "      <td>1209448</td>\n",
       "      <td>TURNER</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>25-SEP-24 21:57:00</td>\n",
       "      <td>25-SEP-24 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710993</th>\n",
       "      <td>2024</td>\n",
       "      <td>1209455</td>\n",
       "      <td>TIFT</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>25-SEP-24 21:57:00</td>\n",
       "      <td>25-SEP-24 23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710994</th>\n",
       "      <td>2024</td>\n",
       "      <td>1209457</td>\n",
       "      <td>TURNER</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>25-SEP-24 23:15:00</td>\n",
       "      <td>26-SEP-24 02:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711074</th>\n",
       "      <td>2024</td>\n",
       "      <td>1209458</td>\n",
       "      <td>WORTH</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>25-SEP-24 23:00:00</td>\n",
       "      <td>26-SEP-24 02:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711075</th>\n",
       "      <td>2024</td>\n",
       "      <td>1209459</td>\n",
       "      <td>WORTH</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>25-SEP-24 23:00:00</td>\n",
       "      <td>26-SEP-24 02:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1815 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        YEAR  EVENT_ID   CZ_NAME    STATE   EVENT_TYPE     BEGIN_DATE_TIME  \\\n",
       "5453    2000   5135328  BEN HILL  GEORGIA  Flash Flood  30-MAR-00 07:00:00   \n",
       "5731    2000   5135327     IRWIN  GEORGIA  Flash Flood  30-MAR-00 07:00:00   \n",
       "5855    2000   5133313     CRISP  GEORGIA  Flash Flood  30-MAR-00 10:30:00   \n",
       "5856    2000   5133314     DOOLY  GEORGIA  Flash Flood  30-MAR-00 10:30:00   \n",
       "5857    2000   5133315    WILCOX  GEORGIA  Flash Flood  30-MAR-00 10:30:00   \n",
       "...      ...       ...       ...      ...          ...                 ...   \n",
       "710912  2024   1209448    TURNER  GEORGIA  Flash Flood  25-SEP-24 21:57:00   \n",
       "710993  2024   1209455      TIFT  GEORGIA  Flash Flood  25-SEP-24 21:57:00   \n",
       "710994  2024   1209457    TURNER  GEORGIA  Flash Flood  25-SEP-24 23:15:00   \n",
       "711074  2024   1209458     WORTH  GEORGIA  Flash Flood  25-SEP-24 23:00:00   \n",
       "711075  2024   1209459     WORTH  GEORGIA  Flash Flood  25-SEP-24 23:00:00   \n",
       "\n",
       "             END_DATE_TIME  \n",
       "5453    30-MAR-00 11:15:00  \n",
       "5731    30-MAR-00 11:15:00  \n",
       "5855    30-MAR-00 10:30:00  \n",
       "5856    30-MAR-00 10:30:00  \n",
       "5857    30-MAR-00 10:30:00  \n",
       "...                    ...  \n",
       "710912  25-SEP-24 23:15:00  \n",
       "710993  25-SEP-24 23:15:00  \n",
       "710994  26-SEP-24 02:15:00  \n",
       "711074  26-SEP-24 02:15:00  \n",
       "711075  26-SEP-24 02:15:00  \n",
       "\n",
       "[1815 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_storm_df = selected_storm_df[[\"YEAR\", \"EVENT_ID\", \"CZ_NAME\", \"STATE\",\"EVENT_TYPE\", \"BEGIN_DATE_TIME\", \"END_DATE_TIME\"]]\n",
    "selected_storm_df = selected_storm_df[\n",
    "    ((selected_storm_df[\"EVENT_TYPE\"] == \"Flood\") | \n",
    "     (selected_storm_df[\"EVENT_TYPE\"] == \"Flash Flood\") | \n",
    "     (selected_storm_df[\"EVENT_TYPE\"] == \"Coastal Flood\")) &\n",
    "    (selected_storm_df[\"STATE\"] == \"GEORGIA\")\n",
    "]\n",
    "selected_storm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_36680/2863138783.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  selected_storm_df[\"year\"] = pd.to_datetime(selected_storm_df[\"BEGIN_DATE_TIME\"], errors=\"coerce\").dt.year\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "event_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "87ff0501-e233-4728-b00d-fbe916f3e965",
       "rows": [
        [
         "0",
         "2000",
         "BALDWIN",
         "1"
        ],
        [
         "1",
         "2000",
         "BARTOW",
         "1"
        ],
        [
         "2",
         "2000",
         "BEN HILL",
         "1"
        ],
        [
         "3",
         "2000",
         "BERRIEN",
         "1"
        ],
        [
         "4",
         "2000",
         "BIBB",
         "2"
        ],
        [
         "5",
         "2000",
         "BROOKS",
         "2"
        ],
        [
         "6",
         "2000",
         "CAMDEN",
         "1"
        ],
        [
         "7",
         "2000",
         "CHEROKEE",
         "1"
        ],
        [
         "8",
         "2000",
         "COBB",
         "1"
        ],
        [
         "9",
         "2000",
         "COFFEE",
         "1"
        ],
        [
         "10",
         "2000",
         "COLQUITT",
         "1"
        ],
        [
         "11",
         "2000",
         "COLUMBIA",
         "2"
        ],
        [
         "12",
         "2000",
         "COWETA",
         "1"
        ],
        [
         "13",
         "2000",
         "CRISP",
         "1"
        ],
        [
         "14",
         "2000",
         "DE KALB",
         "3"
        ],
        [
         "15",
         "2000",
         "DOOLY",
         "1"
        ],
        [
         "16",
         "2000",
         "DOUGHERTY",
         "1"
        ],
        [
         "17",
         "2000",
         "EMANUEL",
         "1"
        ],
        [
         "18",
         "2000",
         "FAYETTE",
         "1"
        ],
        [
         "19",
         "2000",
         "FLOYD",
         "3"
        ],
        [
         "20",
         "2000",
         "FORSYTH",
         "1"
        ],
        [
         "21",
         "2000",
         "FULTON",
         "2"
        ],
        [
         "22",
         "2000",
         "GRADY",
         "1"
        ],
        [
         "23",
         "2000",
         "GWINNETT",
         "1"
        ],
        [
         "24",
         "2000",
         "HALL",
         "1"
        ],
        [
         "25",
         "2000",
         "HARALSON",
         "2"
        ],
        [
         "26",
         "2000",
         "IRWIN",
         "1"
        ],
        [
         "27",
         "2000",
         "LAURENS",
         "1"
        ],
        [
         "28",
         "2000",
         "LOWNDES",
         "2"
        ],
        [
         "29",
         "2000",
         "PAULDING",
         "1"
        ],
        [
         "30",
         "2000",
         "POLK",
         "2"
        ],
        [
         "31",
         "2000",
         "RICHMOND",
         "1"
        ],
        [
         "32",
         "2000",
         "THOMAS",
         "1"
        ],
        [
         "33",
         "2000",
         "WHITE",
         "1"
        ],
        [
         "34",
         "2000",
         "WILCOX",
         "1"
        ],
        [
         "35",
         "2001",
         "BALDWIN",
         "1"
        ],
        [
         "36",
         "2001",
         "BARROW",
         "1"
        ],
        [
         "37",
         "2001",
         "BARTOW",
         "2"
        ],
        [
         "38",
         "2001",
         "BIBB",
         "2"
        ],
        [
         "39",
         "2001",
         "BRANTLEY",
         "1"
        ],
        [
         "40",
         "2001",
         "BROOKS",
         "1"
        ],
        [
         "41",
         "2001",
         "CAMDEN",
         "1"
        ],
        [
         "42",
         "2001",
         "CATOOSA",
         "1"
        ],
        [
         "43",
         "2001",
         "CHEROKEE",
         "4"
        ],
        [
         "44",
         "2001",
         "CLARKE",
         "2"
        ],
        [
         "45",
         "2001",
         "CLINCH",
         "1"
        ],
        [
         "46",
         "2001",
         "COBB",
         "1"
        ],
        [
         "47",
         "2001",
         "COLUMBIA",
         "1"
        ],
        [
         "48",
         "2001",
         "COWETA",
         "1"
        ],
        [
         "49",
         "2001",
         "DOOLY",
         "1"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 728
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>event_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>BALDWIN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>BARTOW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>BEN HILL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>BERRIEN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>BIBB</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>2024</td>\n",
       "      <td>TROUP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>2024</td>\n",
       "      <td>TURNER</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>2024</td>\n",
       "      <td>WALTON</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>2024</td>\n",
       "      <td>WAYNE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>2024</td>\n",
       "      <td>WORTH</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year   CZ_NAME  event_count\n",
       "0    2000   BALDWIN            1\n",
       "1    2000    BARTOW            1\n",
       "2    2000  BEN HILL            1\n",
       "3    2000   BERRIEN            1\n",
       "4    2000      BIBB            2\n",
       "..    ...       ...          ...\n",
       "723  2024     TROUP            1\n",
       "724  2024    TURNER            5\n",
       "725  2024    WALTON            2\n",
       "726  2024     WAYNE            1\n",
       "727  2024     WORTH            8\n",
       "\n",
       "[728 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "selected_storm_df[\"year\"] = pd.to_datetime(selected_storm_df[\"BEGIN_DATE_TIME\"], errors=\"coerce\").dt.year\n",
    "event_counts = selected_storm_df.groupby([\"year\", \"CZ_NAME\"], as_index=False)[\"EVENT_ID\"].count()\n",
    "event_counts.rename(columns={\"EVENT_ID\": \"event_count\"}, inplace=True)\n",
    "\n",
    "event_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_2023",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "event_count_2023",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_2024",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "event_count_2024",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rate_change_percent",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "06c0d3cc-def6-4a4f-8873-8f175bac1b57",
       "rows": [
        [
         "0",
         "2023",
         "BERRIEN",
         "4",
         "2024",
         "8",
         "100.0"
        ],
        [
         "1",
         "2023",
         "BROOKS",
         "4",
         "2024",
         "7",
         "75.0"
        ],
        [
         "2",
         "2023",
         "BULLOCH",
         "1",
         "2024",
         "2",
         "100.0"
        ],
        [
         "3",
         "2023",
         "BURKE",
         "1",
         "2024",
         "12",
         "1100.0"
        ],
        [
         "4",
         "2023",
         "CANDLER",
         "2",
         "2024",
         "1",
         "-50.0"
        ],
        [
         "5",
         "2023",
         "COASTAL GLYNN",
         "3",
         "2024",
         "1",
         "-66.66666666666666"
        ],
        [
         "6",
         "2023",
         "COLQUITT",
         "1",
         "2024",
         "4",
         "300.0"
        ],
        [
         "7",
         "2023",
         "COLUMBIA",
         "1",
         "2024",
         "7",
         "600.0"
        ],
        [
         "8",
         "2023",
         "DOUGHERTY",
         "8",
         "2024",
         "12",
         "50.0"
        ],
        [
         "9",
         "2023",
         "EARLY",
         "3",
         "2024",
         "3",
         "0.0"
        ],
        [
         "10",
         "2023",
         "ELBERT",
         "1",
         "2024",
         "1",
         "0.0"
        ],
        [
         "11",
         "2023",
         "LEE",
         "1",
         "2024",
         "6",
         "500.0"
        ],
        [
         "12",
         "2023",
         "LOWNDES",
         "21",
         "2024",
         "18",
         "-14.285714285714285"
        ],
        [
         "13",
         "2023",
         "MITCHELL",
         "1",
         "2024",
         "21",
         "2000.0"
        ],
        [
         "14",
         "2023",
         "PIKE",
         "18",
         "2024",
         "10",
         "-44.44444444444444"
        ],
        [
         "15",
         "2023",
         "RICHMOND",
         "3",
         "2024",
         "1",
         "-66.66666666666666"
        ],
        [
         "16",
         "2023",
         "TERRELL",
         "1",
         "2024",
         "2",
         "100.0"
        ],
        [
         "17",
         "2023",
         "THOMAS",
         "11",
         "2024",
         "19",
         "72.72727272727273"
        ],
        [
         "18",
         "2023",
         "TROUP",
         "3",
         "2024",
         "1",
         "-66.66666666666666"
        ],
        [
         "19",
         "2023",
         "WALTON",
         "3",
         "2024",
         "2",
         "-33.33333333333333"
        ],
        [
         "20",
         "2023",
         "WAYNE",
         "1",
         "2024",
         "1",
         "0.0"
        ],
        [
         "21",
         "2023",
         "WORTH",
         "7",
         "2024",
         "8",
         "14.285714285714285"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 22
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_2023</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>event_count_2023</th>\n",
       "      <th>year_2024</th>\n",
       "      <th>event_count_2024</th>\n",
       "      <th>rate_change_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>BERRIEN</td>\n",
       "      <td>4</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>BROOKS</td>\n",
       "      <td>4</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>BULLOCH</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>BURKE</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>12</td>\n",
       "      <td>1100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>CANDLER</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>-50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023</td>\n",
       "      <td>COASTAL GLYNN</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>-66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023</td>\n",
       "      <td>COLQUITT</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023</td>\n",
       "      <td>COLUMBIA</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023</td>\n",
       "      <td>DOUGHERTY</td>\n",
       "      <td>8</td>\n",
       "      <td>2024</td>\n",
       "      <td>12</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023</td>\n",
       "      <td>EARLY</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023</td>\n",
       "      <td>ELBERT</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023</td>\n",
       "      <td>LEE</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023</td>\n",
       "      <td>LOWNDES</td>\n",
       "      <td>21</td>\n",
       "      <td>2024</td>\n",
       "      <td>18</td>\n",
       "      <td>-14.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023</td>\n",
       "      <td>MITCHELL</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>21</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023</td>\n",
       "      <td>PIKE</td>\n",
       "      <td>18</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>-44.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023</td>\n",
       "      <td>RICHMOND</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>-66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023</td>\n",
       "      <td>TERRELL</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023</td>\n",
       "      <td>THOMAS</td>\n",
       "      <td>11</td>\n",
       "      <td>2024</td>\n",
       "      <td>19</td>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023</td>\n",
       "      <td>TROUP</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>-66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023</td>\n",
       "      <td>WALTON</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>-33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023</td>\n",
       "      <td>WAYNE</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023</td>\n",
       "      <td>WORTH</td>\n",
       "      <td>7</td>\n",
       "      <td>2024</td>\n",
       "      <td>8</td>\n",
       "      <td>14.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year_2023        CZ_NAME  event_count_2023  year_2024  event_count_2024  \\\n",
       "0        2023        BERRIEN                 4       2024                 8   \n",
       "1        2023         BROOKS                 4       2024                 7   \n",
       "2        2023        BULLOCH                 1       2024                 2   \n",
       "3        2023          BURKE                 1       2024                12   \n",
       "4        2023        CANDLER                 2       2024                 1   \n",
       "5        2023  COASTAL GLYNN                 3       2024                 1   \n",
       "6        2023       COLQUITT                 1       2024                 4   \n",
       "7        2023       COLUMBIA                 1       2024                 7   \n",
       "8        2023      DOUGHERTY                 8       2024                12   \n",
       "9        2023          EARLY                 3       2024                 3   \n",
       "10       2023         ELBERT                 1       2024                 1   \n",
       "11       2023            LEE                 1       2024                 6   \n",
       "12       2023        LOWNDES                21       2024                18   \n",
       "13       2023       MITCHELL                 1       2024                21   \n",
       "14       2023           PIKE                18       2024                10   \n",
       "15       2023       RICHMOND                 3       2024                 1   \n",
       "16       2023        TERRELL                 1       2024                 2   \n",
       "17       2023         THOMAS                11       2024                19   \n",
       "18       2023          TROUP                 3       2024                 1   \n",
       "19       2023         WALTON                 3       2024                 2   \n",
       "20       2023          WAYNE                 1       2024                 1   \n",
       "21       2023          WORTH                 7       2024                 8   \n",
       "\n",
       "    rate_change_percent  \n",
       "0            100.000000  \n",
       "1             75.000000  \n",
       "2            100.000000  \n",
       "3           1100.000000  \n",
       "4            -50.000000  \n",
       "5            -66.666667  \n",
       "6            300.000000  \n",
       "7            600.000000  \n",
       "8             50.000000  \n",
       "9              0.000000  \n",
       "10             0.000000  \n",
       "11           500.000000  \n",
       "12           -14.285714  \n",
       "13          2000.000000  \n",
       "14           -44.444444  \n",
       "15           -66.666667  \n",
       "16           100.000000  \n",
       "17            72.727273  \n",
       "18           -66.666667  \n",
       "19           -33.333333  \n",
       "20             0.000000  \n",
       "21            14.285714  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2023 = event_counts[event_counts[\"year\"] == 2023]\n",
    "count_2024 = event_counts[event_counts[\"year\"] == 2024]\n",
    "\n",
    "merged_counts = pd.merge(count_2023, count_2024, on=\"CZ_NAME\", suffixes=(\"_2023\", \"_2024\"))\n",
    "\n",
    "merged_counts[\"rate_change_percent\"] = ((merged_counts[\"event_count_2024\"] - merged_counts[\"event_count_2023\"]) /\n",
    "                                          merged_counts[\"event_count_2023\"]) * 100\n",
    "\n",
    "merged_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "event_count_2000_2005",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "event_count_2019_2024",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rate_change_percent",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "eafe060e-5660-4627-9b37-e849a9f3aaf4",
       "rows": [
        [
         "0",
         "APPLING",
         "1.0",
         "2.0",
         "100.0"
        ],
        [
         "1",
         "ATKINSON",
         "2.0",
         "2.0",
         "0.0"
        ],
        [
         "2",
         "BACON",
         "3.0",
         "0.0",
         "-100.0"
        ],
        [
         "3",
         "BAKER",
         "1.0",
         "3.0",
         "200.0"
        ],
        [
         "4",
         "BALDWIN",
         "3.0",
         "4.0",
         "33.33333333333333"
        ],
        [
         "5",
         "BANKS",
         "6.0",
         "3.0",
         "-50.0"
        ],
        [
         "6",
         "BARROW",
         "9.0",
         "1.0",
         "-88.88888888888889"
        ],
        [
         "7",
         "BARTOW",
         "14.0",
         "26.0",
         "85.71428571428571"
        ],
        [
         "8",
         "BEN HILL",
         "2.0",
         "9.0",
         "350.0"
        ],
        [
         "9",
         "BERRIEN",
         "2.0",
         "18.0",
         "800.0"
        ],
        [
         "10",
         "BIBB",
         "10.0",
         "3.0",
         "-70.0"
        ],
        [
         "11",
         "BLECKLEY",
         "3.0",
         "0.0",
         "-100.0"
        ],
        [
         "12",
         "BRANTLEY",
         "3.0",
         "1.0",
         "-66.66666666666666"
        ],
        [
         "13",
         "BROOKS",
         "3.0",
         "12.0",
         "300.0"
        ],
        [
         "14",
         "BRYAN",
         "4.0",
         "0.0",
         "-100.0"
        ],
        [
         "15",
         "BULLOCH",
         "7.0",
         "3.0",
         "-57.14285714285714"
        ],
        [
         "16",
         "BURKE",
         "1.0",
         "16.0",
         "1500.0"
        ],
        [
         "17",
         "BUTTS",
         "2.0",
         "1.0",
         "-50.0"
        ],
        [
         "18",
         "CALHOUN",
         "1.0",
         "3.0",
         "200.0"
        ],
        [
         "19",
         "CAMDEN",
         "3.0",
         "3.0",
         "0.0"
        ],
        [
         "20",
         "CANDLER",
         "3.0",
         "3.0",
         "0.0"
        ],
        [
         "21",
         "CARROLL",
         "12.0",
         "1.0",
         "-91.66666666666666"
        ],
        [
         "22",
         "CATOOSA",
         "4.0",
         "2.0",
         "-50.0"
        ],
        [
         "23",
         "CHARLTON",
         "8.0",
         "3.0",
         "-62.5"
        ],
        [
         "24",
         "CHATHAM",
         "7.0",
         "4.0",
         "-42.857142857142854"
        ],
        [
         "25",
         "CHATTAHOOCHEE",
         "2.0",
         "0.0",
         "-100.0"
        ],
        [
         "26",
         "CHATTOOGA",
         "8.0",
         "31.0",
         "287.5"
        ],
        [
         "27",
         "CHEROKEE",
         "17.0",
         "1.0",
         "-94.11764705882352"
        ],
        [
         "28",
         "CLARKE",
         "5.0",
         "1.0",
         "-80.0"
        ],
        [
         "29",
         "CLAYTON",
         "8.0",
         "7.0",
         "-12.5"
        ],
        [
         "30",
         "CLINCH",
         "2.0",
         "1.0",
         "-50.0"
        ],
        [
         "31",
         "COASTAL CAMDEN",
         "1.0",
         "9.0",
         "800.0"
        ],
        [
         "32",
         "COASTAL CHATHAM",
         "0.0",
         "5.0",
         null
        ],
        [
         "33",
         "COASTAL GLYNN",
         "1.0",
         "9.0",
         "800.0"
        ],
        [
         "34",
         "COBB",
         "20.0",
         "5.0",
         "-75.0"
        ],
        [
         "35",
         "COFFEE",
         "4.0",
         "1.0",
         "-75.0"
        ],
        [
         "36",
         "COLQUITT",
         "4.0",
         "7.0",
         "75.0"
        ],
        [
         "37",
         "COLUMBIA",
         "5.0",
         "12.0",
         "140.0"
        ],
        [
         "38",
         "COOK",
         "1.0",
         "5.0",
         "400.0"
        ],
        [
         "39",
         "COWETA",
         "15.0",
         "2.0",
         "-86.66666666666667"
        ],
        [
         "40",
         "CRAWFORD",
         "6.0",
         "3.0",
         "-50.0"
        ],
        [
         "41",
         "CRISP",
         "9.0",
         "7.0",
         "-22.22222222222222"
        ],
        [
         "42",
         "DADE",
         "3.0",
         "6.0",
         "100.0"
        ],
        [
         "43",
         "DAWSON",
         "5.0",
         "5.0",
         "0.0"
        ],
        [
         "44",
         "DE KALB",
         "20.0",
         "10.0",
         "-50.0"
        ],
        [
         "45",
         "DECATUR",
         "2.0",
         "5.0",
         "150.0"
        ],
        [
         "46",
         "DODGE",
         "4.0",
         "5.0",
         "25.0"
        ],
        [
         "47",
         "DOOLY",
         "8.0",
         "1.0",
         "-87.5"
        ],
        [
         "48",
         "DOUGHERTY",
         "1.0",
         "37.0",
         "3600.0"
        ],
        [
         "49",
         "DOUGLAS",
         "11.0",
         "7.0",
         "-36.36363636363637"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 164
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>event_count_2000_2005</th>\n",
       "      <th>event_count_2019_2024</th>\n",
       "      <th>rate_change_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APPLING</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATKINSON</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BACON</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAKER</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BALDWIN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>WHITFIELD</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>137.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>WILCOX</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>WILKES</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>WILKINSON</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>WORTH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CZ_NAME  event_count_2000_2005  event_count_2019_2024  \\\n",
       "0      APPLING                    1.0                    2.0   \n",
       "1     ATKINSON                    2.0                    2.0   \n",
       "2        BACON                    3.0                    0.0   \n",
       "3        BAKER                    1.0                    3.0   \n",
       "4      BALDWIN                    3.0                    4.0   \n",
       "..         ...                    ...                    ...   \n",
       "159  WHITFIELD                    8.0                   19.0   \n",
       "160     WILCOX                    3.0                    3.0   \n",
       "161     WILKES                    3.0                    2.0   \n",
       "162  WILKINSON                    1.0                    1.0   \n",
       "163      WORTH                    1.0                   34.0   \n",
       "\n",
       "     rate_change_percent  \n",
       "0             100.000000  \n",
       "1               0.000000  \n",
       "2            -100.000000  \n",
       "3             200.000000  \n",
       "4              33.333333  \n",
       "..                   ...  \n",
       "159           137.500000  \n",
       "160             0.000000  \n",
       "161           -33.333333  \n",
       "162             0.000000  \n",
       "163          3300.000000  \n",
       "\n",
       "[164 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_two_time_spans(event_counts):\n",
    "  \n",
    "    group1 = event_counts[\n",
    "        event_counts[\"year\"].between(2000, 2005)\n",
    "    ].groupby(\"CZ_NAME\", as_index=False)[\"event_count\"].sum()\n",
    "    group1.rename(columns={\"event_count\": \"event_count_2000_2005\"}, inplace=True)\n",
    "    \n",
    "\n",
    "    group2 = event_counts[\n",
    "        event_counts[\"year\"].between(2019, 2024)\n",
    "    ].groupby(\"CZ_NAME\", as_index=False)[\"event_count\"].sum()\n",
    "    group2.rename(columns={\"event_count\": \"event_count_2019_2024\"}, inplace=True)\n",
    "    \n",
    "    merged_counts = pd.merge(group1, group2, on=\"CZ_NAME\", how=\"outer\")\n",
    "    \n",
    "    # If a CZ_NAME exists in one group but not the other, fill missing counts with 0.\n",
    "    merged_counts[\"event_count_2000_2005\"] = merged_counts[\"event_count_2000_2005\"].fillna(0)\n",
    "    merged_counts[\"event_count_2019_2024\"] = merged_counts[\"event_count_2019_2024\"].fillna(0)\n",
    "    \n",
    "    # If the 2000-2005 count is 0, set it to NaN (avoid division by zero).\n",
    "    merged_counts[\"rate_change_percent\"] = np.where(\n",
    "        merged_counts[\"event_count_2000_2005\"] == 0,\n",
    "        np.nan,\n",
    "        ((merged_counts[\"event_count_2019_2024\"] - merged_counts[\"event_count_2000_2005\"]) \n",
    "          / merged_counts[\"event_count_2000_2005\"]) * 100\n",
    "    )\n",
    "    \n",
    "    return merged_counts\n",
    "\n",
    "final_result = compare_two_time_spans(event_counts)\n",
    "final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv(\"rate_change_per_county_Storm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3824008478.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    " # def load_location_data(folder_path):\n",
    "    \"\"\"\n",
    "    1) Loads all 'StormEvents_locations-ftp_v1.0_dXXXX_*.csv' files from 'folder_path'.\n",
    "    2) Concatenates them into a single DataFrame.\n",
    "    3) Renames each file from 'StormEvents_locations-ftp_v1.0_dYYYY_c...csv'\n",
    "       to 'Location_YYYY.csv'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the pattern to find files\n",
    "    pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    # Load them into a list of DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        location = pd.read_csv(file_path)\n",
    "        dataframes.append(location)\n",
    "\n",
    "    # Rename each file based on the 4-digit year in the pattern 'dYYYY'\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        match = re.search(r\"_d(\\d{4})_\", file_name)\n",
    "        if match:\n",
    "            year = match.group(1)  # e.g., '2018'\n",
    "            new_file_name = f\"Location_{year}.csv\"\n",
    "            new_file_path = os.path.join(folder_path, new_file_name)\n",
    "            \n",
    "            # Rename on disk\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f\"Renamed '{file_name}' -> '{new_file_name}'\")\n",
    "        else:\n",
    "            print(f\"Could not find year in '{file_name}' (no rename done).\")\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    if dataframes:\n",
    "        combined_location = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_location\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024\"\n",
    "location_df = load_location_data(folder)\n",
    "print(location_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EVENT_ID   LOCATION\n",
      "0    285976    GILLHAM\n",
      "1    285978  CENTER PT\n",
      "2    285979   DE QUEEN\n",
      "3    285980   DE QUEEN\n",
      "4    285981       DIAN\n"
     ]
    }
   ],
   "source": [
    "# def select_event_location_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame from your location files,\n",
    "    this function returns a new DataFrame containing only the\n",
    "    'EVENT_ID' and 'LOCATION' columns.\n",
    "    \n",
    "    If the columns are named differently (e.g., 'Location' instead of 'LOCATION'),\n",
    "    update the column names accordingly.\n",
    "    \"\"\"\n",
    "    # Select only the desired columns.\n",
    "    # Adjust the names if your actual DataFrame has a different case/spelling.\n",
    "    selected_df = df[['EVENT_ID', 'LOCATION']]\n",
    "    \n",
    "    return selected_df\n",
    "\n",
    "\n",
    "selected_location_df = select_event_location_columns(location_df)\n",
    "print(selected_location_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location file for year 2007 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2013 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1149701510.py:29: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  storm_df = pd.read_csv(storm_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location file for year 2016 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2003 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2001 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2000 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1988 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1989 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1999 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1998 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1995 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1994 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1996 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1997 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1993 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1987 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1986 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1992 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1990 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1991 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1985 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2019 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2020 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2009 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "     STATE        LOCATION   EVENT_TYPE  EVENT_ID\n",
      "0  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "1  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "2  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "3  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "4     IOWA    NEW HARTFORD        Flood    214762\n"
     ]
    }
   ],
   "source": [
    "# def merge_storm_detail_location(storm_detail_folder, location_folder):\n",
    "    \"\"\"\n",
    "    For each StormDetail_XXXX.csv file in storm_detail_folder, this function:\n",
    "      1. Extracts the year (XXXX) from the filename.\n",
    "      2. Looks for a corresponding Location_XXXX.csv file in location_folder.\n",
    "      3. Loads both CSVs.\n",
    "      4. Merges them on 'EVENT_ID'.\n",
    "      5. Selects only the 'EVENT_ID' and 'location' columns.\n",
    "    \n",
    "    It returns a combined DataFrame with merged data for all years.\n",
    "    \"\"\"\n",
    "    # Pattern for storm detail files: e.g., StormDetail_2000.csv, StormDetail_2001.csv, etc.\n",
    "    storm_pattern = os.path.join(storm_detail_folder, \"StormDetail_*.csv\")\n",
    "    storm_files = glob.glob(storm_pattern)\n",
    "    \n",
    "    merged_list = []\n",
    "    \n",
    "    for storm_file in storm_files:\n",
    "        # Extract the 4-digit year from the filename\n",
    "        base_name = os.path.basename(storm_file)\n",
    "        match = re.search(r\"StormDetail_(\\d{4})\\.csv\", base_name)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            # Construct the expected location file name for that year.\n",
    "            location_file = os.path.join(location_folder, f\"Location_{year}.csv\")\n",
    "            \n",
    "            if os.path.exists(location_file):\n",
    "                # Load the two dataframes.\n",
    "                storm_df = pd.read_csv(storm_file)\n",
    "                location_df = pd.read_csv(location_file)\n",
    "                \n",
    "                # Merge them on EVENT_ID.\n",
    "                merged_df = pd.merge(storm_df, location_df, on='EVENT_ID', how='inner')\n",
    "                \n",
    "                # Select only the EVENT_ID and location columns.\n",
    "                # (Assumes location_df already has the 'location' column; if not, \n",
    "                # you may need to rename the appropriate column first.)\n",
    "                merged_df = merged_df[['STATE', 'LOCATION', 'EVENT_TYPE', 'EVENT_ID']]\n",
    "                \n",
    "                merged_list.append(merged_df)\n",
    "            else:\n",
    "                print(f\"Location file for year {year} not found in {location_folder}.\")\n",
    "        else:\n",
    "            print(f\"Year not found in file name: {base_name}\")\n",
    "    \n",
    "    if merged_list:\n",
    "        # Combine all merged DataFrames into one.\n",
    "        final_merged = pd.concat(merged_list, ignore_index=True)\n",
    "        return final_merged\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "storm_detail_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"\n",
    "location_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024\"\n",
    "\n",
    "merged_data = merge_storm_detail_location(storm_detail_folder, location_folder)\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATE     LOCATION   EVENT_TYPE  EVENT_ID\n",
      "6152  GEORGIA  CENTRAL JCT  Flash Flood    246410\n",
      "6153  GEORGIA  CENTRAL JCT  Flash Flood    246410\n",
      "6154  GEORGIA     SAVANNAH  Flash Flood    246410\n",
      "6155  GEORGIA     SAVANNAH  Flash Flood    246410\n",
      "8623  GEORGIA      CLAXTON  Flash Flood    235603\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE CELL\n",
    "\n",
    "#def filter_storm_data(merged_data):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame so that:\n",
    "      - Only rows where STATE == 'GEORGIA' \n",
    "      - Only rows where EVENT_TYPE is in ['flash flood', 'coastal flood', 'flood']\n",
    "      - Keeps only the columns ['STATE', 'LOCATION_ID']\n",
    "    \"\"\"\n",
    "    # Filter rows\n",
    "    filtered_df = merged_data[\n",
    "        (merged_data[\"STATE\"] == \"GEORGIA\") &\n",
    "        ((merged_data['EVENT_TYPE'] == 'Flash Flood') | \n",
    "              (merged_data['EVENT_TYPE'] == 'Coastal Flood') |\n",
    "              (merged_data['EVENT_TYPE'] == 'Flood') )\n",
    "    ]\n",
    "    \n",
    "    # Select only the desired columns\n",
    "    filtered_df = filtered_df[[\"STATE\", \"LOCATION\", \"EVENT_TYPE\", \"EVENT_ID\"]]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "filtered_storm_df = filter_storm_data(merged_data)\n",
    "print(filtered_storm_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       LOCATION  Event_Count\n",
      "0          (AGS)BUSH FLD AUGUST            1\n",
      "1           (CSG)COLUMBUS METRO            2\n",
      "2          (GVL)GAINESVILLE MEM            2\n",
      "3          (LSF)LAWSON AAF FT B            1\n",
      "4  (MGR)MOULTRIE MUNICIPAL ARPT            5\n"
     ]
    }
   ],
   "source": [
    "# def count_event_ids_by_location(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with 'LOCATION' and 'EVENT_ID' columns,\n",
    "    groups by LOCATION and counts the number of EVENT_ID occurrences.\n",
    "    \n",
    "    Returns a new DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'Event_Count'\n",
    "    \"\"\"\n",
    "    # Group by 'LOCATION' and count the occurrences of 'EVENT_ID'\n",
    "    count_df = df.groupby('LOCATION')['EVENT_ID'].count().reset_index()\n",
    "    count_df.rename(columns={'EVENT_ID': 'Event_Count'}, inplace=True)\n",
    "    return count_df\n",
    "\n",
    "\n",
    "event_counts = count_event_ids_by_location(filtered_storm_df)\n",
    "print(event_counts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Year'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m     merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrate_change\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_year2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_year1\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_year1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_counts\n\u001b[0;32m---> 38\u001b[0m rate_change_df \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_rate_change\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_storm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1985\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(rate_change_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mcalculate_rate_change\u001b[0;34m(filtered_df, year1, year2)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mGiven a DataFrame that includes a 'Year' column along with \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m'LOCATION' and 'EVENT_ID' columns, this function:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  - 'rate_change'\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Filter data for each target year\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m df_year1 \u001b[38;5;241m=\u001b[39m filtered_df[\u001b[43mfiltered_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m year1]\n\u001b[1;32m     20\u001b[0m df_year2 \u001b[38;5;241m=\u001b[39m filtered_df[filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year2]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Count events per location for each year\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Year'"
     ]
    }
   ],
   "source": [
    "\n",
    "# def calculate_rate_change(filtered_df, year1=1985, year2=2023):\n",
    "    \"\"\"\n",
    "    Given a DataFrame that includes a 'Year' column along with \n",
    "    'LOCATION' and 'EVENT_ID' columns, this function:\n",
    "      1. Filters the data to only include rows for year1 and year2.\n",
    "      2. Counts the number of events (EVENT_ID) by LOCATION for each year.\n",
    "      3. Merges the counts (inner join on LOCATION, so only locations present in both years are kept).\n",
    "      4. Calculates the rate of change using:\n",
    "         (count_year2 - count_year1) / count_year1.\n",
    "    \n",
    "    Returns a DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'count_year1'\n",
    "      - 'count_year2'\n",
    "      - 'rate_change'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for each target year\n",
    "    df_year1 = filtered_df[filtered_df['Year'] == year1]\n",
    "    df_year2 = filtered_df[filtered_df['Year'] == year2]\n",
    "    \n",
    "    # Count events per location for each year\n",
    "    count_year1 = df_year1.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year1.rename(columns={'EVENT_ID': 'count_year1'}, inplace=True)\n",
    "    \n",
    "    count_year2 = df_year2.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year2.rename(columns={'EVENT_ID': 'count_year2'}, inplace=True)\n",
    "    \n",
    "    # Merge counts for locations present in both years\n",
    "    merged_counts = pd.merge(count_year1, count_year2, on='LOCATION', how='inner')\n",
    "    \n",
    "    # Calculate the rate of change. If count_year1 is zero, the division will produce inf/NaN.\n",
    "    merged_counts['rate_change'] = (merged_counts['count_year2'] - merged_counts['count_year1']) / merged_counts['count_year1']\n",
    "    \n",
    "    return merged_counts\n",
    "\n",
    "\n",
    "rate_change_df = calculate_rate_change(filtered_storm_df, year1=1985, year2=2023)\n",
    "print(rate_change_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate_change_df.to_csv(\"rate_change_per_location.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
