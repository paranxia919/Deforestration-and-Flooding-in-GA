{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "# Use CZ_NAME in Details -- need cleanning on wording -- as county name\n",
    "# no need to merge location files\n",
    "# upload the 1985-2000 files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1470834919.py:10: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1470834919.py:10: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1470834919.py:10: DtypeWarning: Columns (26,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1470834919.py:10: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1470834919.py:10: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_storm_data():\n",
    "    # Replace 'Storm Data 2000' with the exact folder name you see in VSCode\n",
    "    folder_path = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"  \n",
    "    pattern = os.path.join(folder_path, \"StormDetail_*.csv\")  # This looks for any CSV in that folder\n",
    "    \n",
    "    csv_files = glob.glob(pattern)  # Returns a list of all matching CSV file paths\n",
    "    \n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    if dataframes:\n",
    "        # Combine all individual CSV DataFrames into one big DataFrame\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        # If no files are found or the folder is empty, return an empty DataFrame\n",
    "        return pd.DataFrame()\n",
    "\n",
    "storm_df = load_storm_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1668773538.py:6: DtypeWarning: Columns (26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BEGIN_YEARMONTH",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BEGIN_DAY",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BEGIN_TIME",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "END_YEARMONTH",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "END_DAY",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "END_TIME",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EPISODE_ID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "EVENT_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "STATE_FIPS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "YEAR",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MONTH_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EVENT_TYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CZ_TYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CZ_FIPS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "WFO",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "BEGIN_DATE_TIME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CZ_TIMEZONE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "END_DATE_TIME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "INJURIES_DIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "INJURIES_INDIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DEATHS_DIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DEATHS_INDIRECT",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DAMAGE_PROPERTY",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DAMAGE_CROPS",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SOURCE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MAGNITUDE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAGNITUDE_TYPE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "FLOOD_CAUSE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "CATEGORY",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_F_SCALE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TOR_LENGTH",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_WIDTH",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_OTHER_WFO",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TOR_OTHER_CZ_STATE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "TOR_OTHER_CZ_FIPS",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TOR_OTHER_CZ_NAME",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "BEGIN_RANGE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BEGIN_AZIMUTH",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "BEGIN_LOCATION",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "END_RANGE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "END_AZIMUTH",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "END_LOCATION",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "BEGIN_LAT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "BEGIN_LON",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "END_LAT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "END_LON",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "EPISODE_NARRATIVE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EVENT_NARRATIVE",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "DATA_SOURCE",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c2a2082e-8370-4ced-aa82-6319c923e29d",
       "rows": [
        [
         "112888",
         "202407",
         "4",
         "1500",
         "202407",
         "4",
         "1600",
         "195150.0",
         "1206283",
         "ALABAMA",
         "1",
         "2024",
         "July",
         "Excessive Heat",
         "Z",
         "16",
         "CULLMAN",
         "HUN",
         "04-JUL-24 15:00:00",
         "CST-6",
         "04-JUL-24 16:00:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "AWOS",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Heat index values reached 105 to 109 degrees for much of the late morning and afternoon hours in north central and northwest Alabama. The heat index peaked at 110 to 112 degrees for a few hours during the mid afternoon.",
         "The heat index peaked at 110 degrees for a short period.",
         "CSV"
        ],
        [
         "112889",
         "202407",
         "4",
         "1100",
         "202407",
         "4",
         "1800",
         "195150.0",
         "1206284",
         "ALABAMA",
         "1",
         "2024",
         "July",
         "Heat",
         "Z",
         "16",
         "CULLMAN",
         "HUN",
         "04-JUL-24 11:00:00",
         "CST-6",
         "04-JUL-24 18:00:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "AWOS",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "Heat index values reached 105 to 109 degrees for much of the late morning and afternoon hours in north central and northwest Alabama. The heat index peaked at 110 to 112 degrees for a few hours during the mid afternoon.",
         "The heat index reached 105 to 109 degrees.",
         "CSV"
        ],
        [
         "112890",
         "202407",
         "6",
         "1258",
         "202407",
         "6",
         "1258",
         "192857.0",
         "1198392",
         "NEBRASKA",
         "31",
         "2024",
         "July",
         "Hail",
         "C",
         "47",
         "DAWSON",
         "GID",
         "06-JUL-24 12:58:00",
         "CST-6",
         "06-JUL-24 12:58:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "Public",
         "1.0",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "8.0",
         "SW",
         "LEXINGTON ARPT",
         "8.0",
         "SW",
         "LEXINGTON ARPT",
         "40.71",
         "-99.88",
         "40.71",
         "-99.88",
         "A broad, upper trough was over most of the country, including Nebraska. An upper disturbance passed over the central Plains which created enhanced lift.  CAPE values of around 2,000 J/kg, wind shear of around 40 to 50 knots, and mid-level lapse rates of 7 to 8 degrees C/km were over south-central Nebraska. Temperatures were mostly in the 70s to lower 80s. These conditions resulted in an atmosphere that was conducive for severe storm development.||Thunderstorms had already developed to the northwest of the area during the early to mid-morning hours and moved over central Nebraska during the late morning hours. Additional storms developed to the west and northwest of the area during the late morning hours. These storms moved eastward and one storm began impacting Dawson County with hail up to the size of tennis balls just after 12 PM CDT. This storm continued moving east southeast through the afternoon with other storms moving in behind the original storm. The largest hail of the day was baseball sized and fell in Phelps County after 2 PM CDT. Additional hail impacted Dawson, Gosper, Buffalo, Phelps, Kearney, Franklin, Webster, and Fillmore Counties. These storms produced 5 tornadoes which impacted Kearney, Adams, Clay, and Fillmore Counties. The first two tornadoes impacted Kearney County and were both rated EF1. The third and fourth tornadoes impacted Adams County and were both rated EF0. The fifth tornado first touched down in Clay County then moved into Fillmore County and was rated an EF1. Severe wind gusts were also reported with these storms with the strongest winds estimated near 80 mph in Kearney and Adams Counties. The storm reports ended just before 6 PM CDT as the storms moved east of the area.",
         null,
         "CSV"
        ],
        [
         "112891",
         "202407",
         "6",
         "1523",
         "202407",
         "6",
         "1535",
         "192857.0",
         "1198427",
         "NEBRASKA",
         "31",
         "2024",
         "July",
         "Thunderstorm Wind",
         "C",
         "1",
         "ADAMS",
         "GID",
         "06-JUL-24 15:23:00",
         "CST-6",
         "06-JUL-24 15:35:00",
         "0",
         "0",
         "0",
         "0",
         "15.00K",
         "0.00K",
         "ASOS",
         "60.0",
         "MG",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0",
         "N",
         "(HSI)HASTINGS ARPT",
         "1.0",
         "SE",
         "HASTINGS",
         "40.6",
         "-98.43",
         "40.57",
         "-98.3696",
         "A broad, upper trough was over most of the country, including Nebraska. An upper disturbance passed over the central Plains which created enhanced lift.  CAPE values of around 2,000 J/kg, wind shear of around 40 to 50 knots, and mid-level lapse rates of 7 to 8 degrees C/km were over south-central Nebraska. Temperatures were mostly in the 70s to lower 80s. These conditions resulted in an atmosphere that was conducive for severe storm development.||Thunderstorms had already developed to the northwest of the area during the early to mid-morning hours and moved over central Nebraska during the late morning hours. Additional storms developed to the west and northwest of the area during the late morning hours. These storms moved eastward and one storm began impacting Dawson County with hail up to the size of tennis balls just after 12 PM CDT. This storm continued moving east southeast through the afternoon with other storms moving in behind the original storm. The largest hail of the day was baseball sized and fell in Phelps County after 2 PM CDT. Additional hail impacted Dawson, Gosper, Buffalo, Phelps, Kearney, Franklin, Webster, and Fillmore Counties. These storms produced 5 tornadoes which impacted Kearney, Adams, Clay, and Fillmore Counties. The first two tornadoes impacted Kearney County and were both rated EF1. The third and fourth tornadoes impacted Adams County and were both rated EF0. The fifth tornado first touched down in Clay County then moved into Fillmore County and was rated an EF1. Severe wind gusts were also reported with these storms with the strongest winds estimated near 80 mph in Kearney and Adams Counties. The storm reports ended just before 6 PM CDT as the storms moved east of the area.",
         "A wind gust of 69 MPH was measured by the Hastings Airport ASOS. Generally minor tree damage was reported across Hastings, and power outages were reported in the south part of town.",
         "CSV"
        ],
        [
         "112892",
         "202407",
         "6",
         "1645",
         "202407",
         "6",
         "1645",
         "192857.0",
         "1198773",
         "NEBRASKA",
         "31",
         "2024",
         "July",
         "Hail",
         "C",
         "59",
         "FILLMORE",
         "GID",
         "06-JUL-24 16:45:00",
         "CST-6",
         "06-JUL-24 16:45:00",
         "0",
         "0",
         "0",
         "0",
         "0.00K",
         "0.00K",
         "Public",
         "1.5",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "1.0",
         "NW",
         "GENEVA",
         "1.0",
         "NW",
         "GENEVA",
         "40.54",
         "-97.59",
         "40.54",
         "-97.59",
         "A broad, upper trough was over most of the country, including Nebraska. An upper disturbance passed over the central Plains which created enhanced lift.  CAPE values of around 2,000 J/kg, wind shear of around 40 to 50 knots, and mid-level lapse rates of 7 to 8 degrees C/km were over south-central Nebraska. Temperatures were mostly in the 70s to lower 80s. These conditions resulted in an atmosphere that was conducive for severe storm development.||Thunderstorms had already developed to the northwest of the area during the early to mid-morning hours and moved over central Nebraska during the late morning hours. Additional storms developed to the west and northwest of the area during the late morning hours. These storms moved eastward and one storm began impacting Dawson County with hail up to the size of tennis balls just after 12 PM CDT. This storm continued moving east southeast through the afternoon with other storms moving in behind the original storm. The largest hail of the day was baseball sized and fell in Phelps County after 2 PM CDT. Additional hail impacted Dawson, Gosper, Buffalo, Phelps, Kearney, Franklin, Webster, and Fillmore Counties. These storms produced 5 tornadoes which impacted Kearney, Adams, Clay, and Fillmore Counties. The first two tornadoes impacted Kearney County and were both rated EF1. The third and fourth tornadoes impacted Adams County and were both rated EF0. The fifth tornado first touched down in Clay County then moved into Fillmore County and was rated an EF1. Severe wind gusts were also reported with these storms with the strongest winds estimated near 80 mph in Kearney and Adams Counties. The storm reports ended just before 6 PM CDT as the storms moved east of the area.",
         null,
         "CSV"
        ]
       ],
       "shape": {
        "columns": 51,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112888</th>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1500</td>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1600</td>\n",
       "      <td>195150.0</td>\n",
       "      <td>1206283</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heat index values reached 105 to 109 degrees f...</td>\n",
       "      <td>The heat index peaked at 110 degrees for a sho...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112889</th>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1100</td>\n",
       "      <td>202407</td>\n",
       "      <td>4</td>\n",
       "      <td>1800</td>\n",
       "      <td>195150.0</td>\n",
       "      <td>1206284</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heat index values reached 105 to 109 degrees f...</td>\n",
       "      <td>The heat index reached 105 to 109 degrees.</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112890</th>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1258</td>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1258</td>\n",
       "      <td>192857.0</td>\n",
       "      <td>1198392</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>SW</td>\n",
       "      <td>LEXINGTON ARPT</td>\n",
       "      <td>40.71</td>\n",
       "      <td>-99.88</td>\n",
       "      <td>40.71</td>\n",
       "      <td>-99.8800</td>\n",
       "      <td>A broad, upper trough was over most of the cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112891</th>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1523</td>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1535</td>\n",
       "      <td>192857.0</td>\n",
       "      <td>1198427</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>HASTINGS</td>\n",
       "      <td>40.60</td>\n",
       "      <td>-98.43</td>\n",
       "      <td>40.57</td>\n",
       "      <td>-98.3696</td>\n",
       "      <td>A broad, upper trough was over most of the cou...</td>\n",
       "      <td>A wind gust of 69 MPH was measured by the Hast...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112892</th>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1645</td>\n",
       "      <td>202407</td>\n",
       "      <td>6</td>\n",
       "      <td>1645</td>\n",
       "      <td>192857.0</td>\n",
       "      <td>1198773</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>GENEVA</td>\n",
       "      <td>40.54</td>\n",
       "      <td>-97.59</td>\n",
       "      <td>40.54</td>\n",
       "      <td>-97.5900</td>\n",
       "      <td>A broad, upper trough was over most of the cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  \\\n",
       "112888           202407          4        1500         202407        4   \n",
       "112889           202407          4        1100         202407        4   \n",
       "112890           202407          6        1258         202407        6   \n",
       "112891           202407          6        1523         202407        6   \n",
       "112892           202407          6        1645         202407        6   \n",
       "\n",
       "        END_TIME  EPISODE_ID  EVENT_ID     STATE  STATE_FIPS  ...  END_RANGE  \\\n",
       "112888      1600    195150.0   1206283   ALABAMA           1  ...        NaN   \n",
       "112889      1800    195150.0   1206284   ALABAMA           1  ...        NaN   \n",
       "112890      1258    192857.0   1198392  NEBRASKA          31  ...        8.0   \n",
       "112891      1535    192857.0   1198427  NEBRASKA          31  ...        1.0   \n",
       "112892      1645    192857.0   1198773  NEBRASKA          31  ...        1.0   \n",
       "\n",
       "       END_AZIMUTH    END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT  END_LON  \\\n",
       "112888         NaN             NaN       NaN        NaN     NaN      NaN   \n",
       "112889         NaN             NaN       NaN        NaN     NaN      NaN   \n",
       "112890          SW  LEXINGTON ARPT     40.71     -99.88   40.71 -99.8800   \n",
       "112891          SE        HASTINGS     40.60     -98.43   40.57 -98.3696   \n",
       "112892          NW          GENEVA     40.54     -97.59   40.54 -97.5900   \n",
       "\n",
       "                                        EPISODE_NARRATIVE  \\\n",
       "112888  Heat index values reached 105 to 109 degrees f...   \n",
       "112889  Heat index values reached 105 to 109 degrees f...   \n",
       "112890  A broad, upper trough was over most of the cou...   \n",
       "112891  A broad, upper trough was over most of the cou...   \n",
       "112892  A broad, upper trough was over most of the cou...   \n",
       "\n",
       "                                          EVENT_NARRATIVE DATA_SOURCE  \n",
       "112888  The heat index peaked at 110 degrees for a sho...         CSV  \n",
       "112889         The heat index reached 105 to 109 degrees.         CSV  \n",
       "112890                                                NaN         CSV  \n",
       "112891  A wind gust of 69 MPH was measured by the Hast...         CSV  \n",
       "112892                                                NaN         CSV  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_selected_storm_data(folder_path, years=[1996, 2024]):\n",
    "    dataframes = []\n",
    "    for year in years:\n",
    "        file_path = os.path.join(folder_path, f\"StormDetail_{year}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"File for year {year} does not exist.\")\n",
    "    \n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Usage\n",
    "folder_path = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"\n",
    "selected_storm_df = load_selected_storm_data(folder_path)\n",
    "\n",
    "selected_storm_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "YEAR",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EVENT_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EVENT_TYPE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "BEGIN_DATE_TIME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "END_DATE_TIME",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "be426c6a-6368-4593-8a7d-e1f4a78014eb",
       "rows": [
        [
         "1044",
         "1996",
         "5536010",
         "COBB",
         "GEORGIA",
         "Flash Flood",
         "26-JAN-96 22:55:00",
         "26-JAN-96 22:55:00"
        ],
        [
         "1460",
         "1996",
         "5536004",
         "GILMER",
         "GEORGIA",
         "Flash Flood",
         "26-JAN-96 19:00:00",
         "26-JAN-96 21:00:00"
        ],
        [
         "1466",
         "1996",
         "5536005",
         "CATOOSA",
         "GEORGIA",
         "Flash Flood",
         "26-JAN-96 20:50:00",
         "26-JAN-96 20:50:00"
        ],
        [
         "1467",
         "1996",
         "5536006",
         "WHITE",
         "GEORGIA",
         "Flash Flood",
         "26-JAN-96 21:00:00",
         "26-JAN-96 21:00:00"
        ],
        [
         "1755",
         "1996",
         "5536011",
         "UNION",
         "GEORGIA",
         "Flash Flood",
         "26-JAN-96 23:00:00",
         "27-JAN-96 00:00:00"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>1996</td>\n",
       "      <td>5536010</td>\n",
       "      <td>COBB</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>26-JAN-96 22:55:00</td>\n",
       "      <td>26-JAN-96 22:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>1996</td>\n",
       "      <td>5536004</td>\n",
       "      <td>GILMER</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>26-JAN-96 19:00:00</td>\n",
       "      <td>26-JAN-96 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>1996</td>\n",
       "      <td>5536005</td>\n",
       "      <td>CATOOSA</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>26-JAN-96 20:50:00</td>\n",
       "      <td>26-JAN-96 20:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1996</td>\n",
       "      <td>5536006</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>26-JAN-96 21:00:00</td>\n",
       "      <td>26-JAN-96 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>1996</td>\n",
       "      <td>5536011</td>\n",
       "      <td>UNION</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Flash Flood</td>\n",
       "      <td>26-JAN-96 23:00:00</td>\n",
       "      <td>27-JAN-96 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      YEAR  EVENT_ID  CZ_NAME    STATE   EVENT_TYPE     BEGIN_DATE_TIME  \\\n",
       "1044  1996   5536010     COBB  GEORGIA  Flash Flood  26-JAN-96 22:55:00   \n",
       "1460  1996   5536004   GILMER  GEORGIA  Flash Flood  26-JAN-96 19:00:00   \n",
       "1466  1996   5536005  CATOOSA  GEORGIA  Flash Flood  26-JAN-96 20:50:00   \n",
       "1467  1996   5536006    WHITE  GEORGIA  Flash Flood  26-JAN-96 21:00:00   \n",
       "1755  1996   5536011    UNION  GEORGIA  Flash Flood  26-JAN-96 23:00:00   \n",
       "\n",
       "           END_DATE_TIME  \n",
       "1044  26-JAN-96 22:55:00  \n",
       "1460  26-JAN-96 21:00:00  \n",
       "1466  26-JAN-96 20:50:00  \n",
       "1467  26-JAN-96 21:00:00  \n",
       "1755  27-JAN-96 00:00:00  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_storm_df = selected_storm_df[[\"YEAR\", \"EVENT_ID\", \"CZ_NAME\", \"STATE\",\"EVENT_TYPE\", \"BEGIN_DATE_TIME\", \"END_DATE_TIME\"]]\n",
    "selected_storm_df = selected_storm_df[\n",
    "    ((selected_storm_df[\"EVENT_TYPE\"] == \"Flood\") | \n",
    "     (selected_storm_df[\"EVENT_TYPE\"] == \"Flash Flood\") | \n",
    "     (selected_storm_df[\"EVENT_TYPE\"] == \"Coastal Flood\")) &\n",
    "    (selected_storm_df[\"STATE\"] == \"GEORGIA\")\n",
    "]\n",
    "selected_storm_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/2444689808.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  selected_storm_df[\"year\"] = pd.to_datetime(selected_storm_df[\"BEGIN_DATE_TIME\"], errors=\"coerce\").dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year   CZ_NAME  event_count\n",
      "0   1996     BUTTS            1\n",
      "1   1996    CAMDEN            1\n",
      "2   1996   CARROLL            1\n",
      "3   1996   CATOOSA            1\n",
      "4   1996  CHARLTON            2\n",
      "..   ...       ...          ...\n",
      "84  2024     TROUP            1\n",
      "85  2024    TURNER            5\n",
      "86  2024    WALTON            2\n",
      "87  2024     WAYNE            1\n",
      "88  2024     WORTH            8\n",
      "\n",
      "[89 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_storm_df[\"year\"] = pd.to_datetime(selected_storm_df[\"BEGIN_DATE_TIME\"], errors=\"coerce\").dt.year\n",
    "\n",
    "# Group by 'year' and 'CZ_NAME' and count the number of events (using EVENT_ID)\n",
    "event_counts = selected_storm_df.groupby([\"year\", \"CZ_NAME\"], as_index=False)[\"EVENT_ID\"].count()\n",
    "event_counts.rename(columns={\"EVENT_ID\": \"event_count\"}, inplace=True)\n",
    "\n",
    "print(event_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_1996",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "CZ_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "event_count_1996",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_2024",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "event_count_2024",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rate_change_percent",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0bb12180-9949-4914-8085-efc3479d0f9c",
       "rows": [
        [
         "0",
         "1996",
         "CAMDEN",
         "1",
         "2024",
         "3",
         "200.0"
        ],
        [
         "1",
         "1996",
         "CHATHAM",
         "2",
         "2024",
         "4",
         "100.0"
        ],
        [
         "2",
         "1996",
         "COLUMBIA",
         "2",
         "2024",
         "7",
         "250.0"
        ],
        [
         "3",
         "1996",
         "ELBERT",
         "1",
         "2024",
         "1",
         "0.0"
        ],
        [
         "4",
         "1996",
         "HABERSHAM",
         "4",
         "2024",
         "3",
         "-25.0"
        ],
        [
         "5",
         "1996",
         "PIKE",
         "1",
         "2024",
         "10",
         "900.0"
        ],
        [
         "6",
         "1996",
         "RABUN",
         "3",
         "2024",
         "2",
         "-33.33333333333333"
        ],
        [
         "7",
         "1996",
         "RICHMOND",
         "1",
         "2024",
         "1",
         "0.0"
        ],
        [
         "8",
         "1996",
         "SPALDING",
         "1",
         "2024",
         "1",
         "0.0"
        ],
        [
         "9",
         "1996",
         "TALBOT",
         "1",
         "2024",
         "2",
         "100.0"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_1996</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>event_count_1996</th>\n",
       "      <th>year_2024</th>\n",
       "      <th>event_count_2024</th>\n",
       "      <th>rate_change_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996</td>\n",
       "      <td>CAMDEN</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996</td>\n",
       "      <td>CHATHAM</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996</td>\n",
       "      <td>COLUMBIA</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>7</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996</td>\n",
       "      <td>ELBERT</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996</td>\n",
       "      <td>HABERSHAM</td>\n",
       "      <td>4</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>-25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>PIKE</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1996</td>\n",
       "      <td>RABUN</td>\n",
       "      <td>3</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>-33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1996</td>\n",
       "      <td>RICHMOND</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1996</td>\n",
       "      <td>SPALDING</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1996</td>\n",
       "      <td>TALBOT</td>\n",
       "      <td>1</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_1996    CZ_NAME  event_count_1996  year_2024  event_count_2024  \\\n",
       "0       1996     CAMDEN                 1       2024                 3   \n",
       "1       1996    CHATHAM                 2       2024                 4   \n",
       "2       1996   COLUMBIA                 2       2024                 7   \n",
       "3       1996     ELBERT                 1       2024                 1   \n",
       "4       1996  HABERSHAM                 4       2024                 3   \n",
       "5       1996       PIKE                 1       2024                10   \n",
       "6       1996      RABUN                 3       2024                 2   \n",
       "7       1996   RICHMOND                 1       2024                 1   \n",
       "8       1996   SPALDING                 1       2024                 1   \n",
       "9       1996     TALBOT                 1       2024                 2   \n",
       "\n",
       "   rate_change_percent  \n",
       "0           200.000000  \n",
       "1           100.000000  \n",
       "2           250.000000  \n",
       "3             0.000000  \n",
       "4           -25.000000  \n",
       "5           900.000000  \n",
       "6           -33.333333  \n",
       "7             0.000000  \n",
       "8             0.000000  \n",
       "9           100.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1996 = event_counts[event_counts[\"year\"] == 1996]\n",
    "count_2024 = event_counts[event_counts[\"year\"] == 2024]\n",
    "\n",
    "merged_counts = pd.merge(count_1996, count_2024, on=\"CZ_NAME\", suffixes=(\"_1996\", \"_2024\"))\n",
    "\n",
    "merged_counts[\"rate_change_percent\"] = ((merged_counts[\"event_count_2024\"] - merged_counts[\"event_count_1996\"]) /\n",
    "                                          merged_counts[\"event_count_1996\"]) * 100\n",
    "\n",
    "merged_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts.to_csv(\"rate_change_per_county_Storm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find year in 'Location_2011.csv' (no rename done).\n",
      "Could not find year in 'Location_2005.csv' (no rename done).\n",
      "Could not find year in 'Location_2004.csv' (no rename done).\n",
      "Could not find year in 'Location_2010.csv' (no rename done).\n",
      "Could not find year in 'Location_2006.csv' (no rename done).\n",
      "Could not find year in 'Location_2012.csv' (no rename done).\n",
      "Could not find year in 'Location_2017.csv' (no rename done).\n",
      "Could not find year in 'Location_2002.csv' (no rename done).\n",
      "Could not find year in 'Location_2014.csv' (no rename done).\n",
      "Could not find year in 'Location_2015.csv' (no rename done).\n",
      "Could not find year in 'Location_2024.csv' (no rename done).\n",
      "Could not find year in 'Location_2018.csv' (no rename done).\n",
      "Could not find year in 'Location_2022.csv' (no rename done).\n",
      "Could not find year in 'Location_2023.csv' (no rename done).\n",
      "Could not find year in 'Location_2021.csv' (no rename done).\n",
      "Could not find year in 'Location_2008.csv' (no rename done).\n",
      "   YEARMONTH  EPISODE_ID  EVENT_ID  LOCATION_INDEX  RANGE AZIMUTH   LOCATION  \\\n",
      "0     201104       48862    285976               1   0.57       E    GILLHAM   \n",
      "1     201104       48862    285978               1   2.39     ESE  CENTER PT   \n",
      "2     201104       48862    285979               1   0.00       N   DE QUEEN   \n",
      "3     201104       48862    285980               1   0.90      NW   DE QUEEN   \n",
      "4     201104       48862    285981               1   0.57       E       DIAN   \n",
      "\n",
      "   LATITUDE  LONGITUDE       LAT2       LON2  \n",
      "0     34.17     -94.31  3410200.0  9418600.0  \n",
      "1     34.02     -93.89   341200.0  9353400.0  \n",
      "2     34.03     -94.33   341800.0  9419800.0  \n",
      "3     34.04     -94.34   342400.0  9420400.0  \n",
      "4     33.80     -93.39  3348000.0  9323400.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    " # def load_location_data(folder_path):\n",
    "    \"\"\"\n",
    "    1) Loads all 'StormEvents_locations-ftp_v1.0_dXXXX_*.csv' files from 'folder_path'.\n",
    "    2) Concatenates them into a single DataFrame.\n",
    "    3) Renames each file from 'StormEvents_locations-ftp_v1.0_dYYYY_c...csv'\n",
    "       to 'Location_YYYY.csv'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the pattern to find files\n",
    "    pattern = os.path.join(folder_path, \"*.csv\")\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    # Load them into a list of DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        location = pd.read_csv(file_path)\n",
    "        dataframes.append(location)\n",
    "\n",
    "    # Rename each file based on the 4-digit year in the pattern 'dYYYY'\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        match = re.search(r\"_d(\\d{4})_\", file_name)\n",
    "        if match:\n",
    "            year = match.group(1)  # e.g., '2018'\n",
    "            new_file_name = f\"Location_{year}.csv\"\n",
    "            new_file_path = os.path.join(folder_path, new_file_name)\n",
    "            \n",
    "            # Rename on disk\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f\"Renamed '{file_name}' -> '{new_file_name}'\")\n",
    "        else:\n",
    "            print(f\"Could not find year in '{file_name}' (no rename done).\")\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    if dataframes:\n",
    "        combined_location = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_location\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024\"\n",
    "location_df = load_location_data(folder)\n",
    "print(location_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EVENT_ID   LOCATION\n",
      "0    285976    GILLHAM\n",
      "1    285978  CENTER PT\n",
      "2    285979   DE QUEEN\n",
      "3    285980   DE QUEEN\n",
      "4    285981       DIAN\n"
     ]
    }
   ],
   "source": [
    "# def select_event_location_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame from your location files,\n",
    "    this function returns a new DataFrame containing only the\n",
    "    'EVENT_ID' and 'LOCATION' columns.\n",
    "    \n",
    "    If the columns are named differently (e.g., 'Location' instead of 'LOCATION'),\n",
    "    update the column names accordingly.\n",
    "    \"\"\"\n",
    "    # Select only the desired columns.\n",
    "    # Adjust the names if your actual DataFrame has a different case/spelling.\n",
    "    selected_df = df[['EVENT_ID', 'LOCATION']]\n",
    "    \n",
    "    return selected_df\n",
    "\n",
    "\n",
    "selected_location_df = select_event_location_columns(location_df)\n",
    "print(selected_location_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location file for year 2007 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2013 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_40795/1149701510.py:29: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  storm_df = pd.read_csv(storm_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location file for year 2016 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2003 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2001 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2000 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1988 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1989 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1999 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1998 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1995 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1994 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1996 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1997 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1993 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1987 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1986 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1992 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1990 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1991 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 1985 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2019 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2020 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "Location file for year 2009 not found in /Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024.\n",
      "     STATE        LOCATION   EVENT_TYPE  EVENT_ID\n",
      "0  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "1  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "2  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "3  FLORIDA  SOUTH FLOMATON  Flash Flood    213816\n",
      "4     IOWA    NEW HARTFORD        Flood    214762\n"
     ]
    }
   ],
   "source": [
    "# def merge_storm_detail_location(storm_detail_folder, location_folder):\n",
    "    \"\"\"\n",
    "    For each StormDetail_XXXX.csv file in storm_detail_folder, this function:\n",
    "      1. Extracts the year (XXXX) from the filename.\n",
    "      2. Looks for a corresponding Location_XXXX.csv file in location_folder.\n",
    "      3. Loads both CSVs.\n",
    "      4. Merges them on 'EVENT_ID'.\n",
    "      5. Selects only the 'EVENT_ID' and 'location' columns.\n",
    "    \n",
    "    It returns a combined DataFrame with merged data for all years.\n",
    "    \"\"\"\n",
    "    # Pattern for storm detail files: e.g., StormDetail_2000.csv, StormDetail_2001.csv, etc.\n",
    "    storm_pattern = os.path.join(storm_detail_folder, \"StormDetail_*.csv\")\n",
    "    storm_files = glob.glob(storm_pattern)\n",
    "    \n",
    "    merged_list = []\n",
    "    \n",
    "    for storm_file in storm_files:\n",
    "        # Extract the 4-digit year from the filename\n",
    "        base_name = os.path.basename(storm_file)\n",
    "        match = re.search(r\"StormDetail_(\\d{4})\\.csv\", base_name)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            # Construct the expected location file name for that year.\n",
    "            location_file = os.path.join(location_folder, f\"Location_{year}.csv\")\n",
    "            \n",
    "            if os.path.exists(location_file):\n",
    "                # Load the two dataframes.\n",
    "                storm_df = pd.read_csv(storm_file)\n",
    "                location_df = pd.read_csv(location_file)\n",
    "                \n",
    "                # Merge them on EVENT_ID.\n",
    "                merged_df = pd.merge(storm_df, location_df, on='EVENT_ID', how='inner')\n",
    "                \n",
    "                # Select only the EVENT_ID and location columns.\n",
    "                # (Assumes location_df already has the 'location' column; if not, \n",
    "                # you may need to rename the appropriate column first.)\n",
    "                merged_df = merged_df[['STATE', 'LOCATION', 'EVENT_TYPE', 'EVENT_ID']]\n",
    "                \n",
    "                merged_list.append(merged_df)\n",
    "            else:\n",
    "                print(f\"Location file for year {year} not found in {location_folder}.\")\n",
    "        else:\n",
    "            print(f\"Year not found in file name: {base_name}\")\n",
    "    \n",
    "    if merged_list:\n",
    "        # Combine all merged DataFrames into one.\n",
    "        final_merged = pd.concat(merged_list, ignore_index=True)\n",
    "        return final_merged\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "storm_detail_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 1985-2024\"\n",
    "location_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 1985-2024\"\n",
    "\n",
    "merged_data = merge_storm_detail_location(storm_detail_folder, location_folder)\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATE     LOCATION   EVENT_TYPE  EVENT_ID\n",
      "6152  GEORGIA  CENTRAL JCT  Flash Flood    246410\n",
      "6153  GEORGIA  CENTRAL JCT  Flash Flood    246410\n",
      "6154  GEORGIA     SAVANNAH  Flash Flood    246410\n",
      "6155  GEORGIA     SAVANNAH  Flash Flood    246410\n",
      "8623  GEORGIA      CLAXTON  Flash Flood    235603\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE CELL\n",
    "\n",
    "#def filter_storm_data(merged_data):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame so that:\n",
    "      - Only rows where STATE == 'GEORGIA' \n",
    "      - Only rows where EVENT_TYPE is in ['flash flood', 'coastal flood', 'flood']\n",
    "      - Keeps only the columns ['STATE', 'LOCATION_ID']\n",
    "    \"\"\"\n",
    "    # Filter rows\n",
    "    filtered_df = merged_data[\n",
    "        (merged_data[\"STATE\"] == \"GEORGIA\") &\n",
    "        ((merged_data['EVENT_TYPE'] == 'Flash Flood') | \n",
    "              (merged_data['EVENT_TYPE'] == 'Coastal Flood') |\n",
    "              (merged_data['EVENT_TYPE'] == 'Flood') )\n",
    "    ]\n",
    "    \n",
    "    # Select only the desired columns\n",
    "    filtered_df = filtered_df[[\"STATE\", \"LOCATION\", \"EVENT_TYPE\", \"EVENT_ID\"]]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "filtered_storm_df = filter_storm_data(merged_data)\n",
    "print(filtered_storm_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       LOCATION  Event_Count\n",
      "0          (AGS)BUSH FLD AUGUST            1\n",
      "1           (CSG)COLUMBUS METRO            2\n",
      "2          (GVL)GAINESVILLE MEM            2\n",
      "3          (LSF)LAWSON AAF FT B            1\n",
      "4  (MGR)MOULTRIE MUNICIPAL ARPT            5\n"
     ]
    }
   ],
   "source": [
    "# def count_event_ids_by_location(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with 'LOCATION' and 'EVENT_ID' columns,\n",
    "    groups by LOCATION and counts the number of EVENT_ID occurrences.\n",
    "    \n",
    "    Returns a new DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'Event_Count'\n",
    "    \"\"\"\n",
    "    # Group by 'LOCATION' and count the occurrences of 'EVENT_ID'\n",
    "    count_df = df.groupby('LOCATION')['EVENT_ID'].count().reset_index()\n",
    "    count_df.rename(columns={'EVENT_ID': 'Event_Count'}, inplace=True)\n",
    "    return count_df\n",
    "\n",
    "\n",
    "event_counts = count_event_ids_by_location(filtered_storm_df)\n",
    "print(event_counts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Year'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m     merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrate_change\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_year2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_year1\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m merged_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_year1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_counts\n\u001b[0;32m---> 38\u001b[0m rate_change_df \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_rate_change\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_storm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1985\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(rate_change_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mcalculate_rate_change\u001b[0;34m(filtered_df, year1, year2)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mGiven a DataFrame that includes a 'Year' column along with \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m'LOCATION' and 'EVENT_ID' columns, this function:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  - 'rate_change'\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Filter data for each target year\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m df_year1 \u001b[38;5;241m=\u001b[39m filtered_df[\u001b[43mfiltered_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m year1]\n\u001b[1;32m     20\u001b[0m df_year2 \u001b[38;5;241m=\u001b[39m filtered_df[filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m year2]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Count events per location for each year\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Year'"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_rate_change(filtered_df, year1=1985, year2=2023):\n",
    "    \"\"\"\n",
    "    Given a DataFrame that includes a 'Year' column along with \n",
    "    'LOCATION' and 'EVENT_ID' columns, this function:\n",
    "      1. Filters the data to only include rows for year1 and year2.\n",
    "      2. Counts the number of events (EVENT_ID) by LOCATION for each year.\n",
    "      3. Merges the counts (inner join on LOCATION, so only locations present in both years are kept).\n",
    "      4. Calculates the rate of change using:\n",
    "         (count_year2 - count_year1) / count_year1.\n",
    "    \n",
    "    Returns a DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'count_year1'\n",
    "      - 'count_year2'\n",
    "      - 'rate_change'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for each target year\n",
    "    df_year1 = filtered_df[filtered_df['Year'] == year1]\n",
    "    df_year2 = filtered_df[filtered_df['Year'] == year2]\n",
    "    \n",
    "    # Count events per location for each year\n",
    "    count_year1 = df_year1.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year1.rename(columns={'EVENT_ID': 'count_year1'}, inplace=True)\n",
    "    \n",
    "    count_year2 = df_year2.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year2.rename(columns={'EVENT_ID': 'count_year2'}, inplace=True)\n",
    "    \n",
    "    # Merge counts for locations present in both years\n",
    "    merged_counts = pd.merge(count_year1, count_year2, on='LOCATION', how='inner')\n",
    "    \n",
    "    # Calculate the rate of change. If count_year1 is zero, the division will produce inf/NaN.\n",
    "    merged_counts['rate_change'] = (merged_counts['count_year2'] - merged_counts['count_year1']) / merged_counts['count_year1']\n",
    "    \n",
    "    return merged_counts\n",
    "\n",
    "\n",
    "rate_change_df = calculate_rate_change(filtered_storm_df, year1=1985, year2=2023)\n",
    "print(rate_change_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_change_df.to_csv(\"rate_change_per_location.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
