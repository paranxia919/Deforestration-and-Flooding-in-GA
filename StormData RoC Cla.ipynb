{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/gmc62dp11n708wvtvkhzfldc0000gn/T/ipykernel_19653/1902818050.py:10: DtypeWarning: Columns (29,34,35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_storm_data():\n",
    "    # Replace 'Storm Data 2000' with the exact folder name you see in VSCode\n",
    "    folder_path = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 2000-2024\"  \n",
    "    pattern = os.path.join(folder_path, \"*.csv\")  # This looks for any CSV in that folder\n",
    "    \n",
    "    csv_files = glob.glob(pattern)  # Returns a list of all matching CSV file paths\n",
    "    \n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    if dataframes:\n",
    "        # Combine all individual CSV DataFrames into one big DataFrame\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        # If no files are found or the folder is empty, return an empty DataFrame\n",
    "        return pd.DataFrame()\n",
    "\n",
    "storm_df = load_storm_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rename_storm_files(folder_path):\n",
    "    \"\"\"\n",
    "    Renames files in 'folder_path' from:\n",
    "      StormEvents_details-ftp_v1.0_d2000_c20220425.csv\n",
    "    to:\n",
    "      StormDetail_2000.csv\n",
    "    \n",
    "    Assumes each filename contains '_dXXXX_' where XXXX is the year.\n",
    "    \"\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Only consider CSVs that match the general pattern\n",
    "        if file_name.startswith(\"StormEvents_details-ftp_v1.0_d\") and file_name.endswith(\".csv\"):\n",
    "            # Extract the 4-digit year from something like '_d2000_'\n",
    "            match = re.search(r\"_d(\\d{4})_\", file_name)\n",
    "            if match:\n",
    "                year = match.group(1)  # e.g. '2000'\n",
    "                # Build your new file name\n",
    "                new_name = f\"StormDetail_{year}.csv\"\n",
    "                \n",
    "                old_path = os.path.join(folder_path, file_name)\n",
    "                new_path = os.path.join(folder_path, new_name)\n",
    "                \n",
    "                # Rename the file\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"Renamed '{file_name}' -> '{new_name}'\")\n",
    "\n",
    "# Example usage:\n",
    "# rename_storm_files(\"/path/to/your/folder/with/csvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "['StormEvents_details-ftp_v1.0_d2014_c20231116.csv', 'StormEvents_details-ftp_v1.0_d2000_c20220425.csv', 'StormEvents_details-ftp_v1.0_d2006_c20250122.csv', 'StormEvents_details-ftp_v1.0_d2011_c20230417.csv', 'StormEvents_details-ftp_v1.0_d2004_c20220425.csv', '.DS_Store', 'StormEvents_details-ftp_v1.0_d2003_c20220425.csv', 'StormEvents_details-ftp_v1.0_d2010_c20220425.csv', 'StormEvents_details-ftp_v1.0_d2008_c20240620.csv', 'StormEvents_details-ftp_v1.0_d2007_c20240216.csv', 'StormEvents_details-ftp_v1.0_d2019_c20240117.csv', 'StormEvents_details-ftp_v1.0_d2013_c20230118.csv', 'StormEvents_details-ftp_v1.0_d2009_c20231116.csv', 'StormEvents_details-ftp_v1.0_d2021_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2018_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2023_c20241216.csv', 'StormEvents_details-ftp_v1.0_d2012_c20221216.csv', 'StormEvents_details-ftp_v1.0_d2016_c20220719.csv', 'StormEvents_details-ftp_v1.0_d2024_c20250122.csv', 'StormEvents_details-ftp_v1.0_d2017_c20250122.csv', 'StormEvents_details-ftp_v1.0_d2002_c20220425.csv', 'StormEvents_details-ftp_v1.0_d2015_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2022_c20241121.csv', 'StormEvents_details-ftp_v1.0_d2020_c20240620.csv', 'StormEvents_details-ftp_v1.0_d2005_c20220425.csv', 'StormEvents_details-ftp_v1.0_d2001_c20220425.csv']\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2014_c20231116.csv' -> 'StormDetail_2014.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2000_c20220425.csv' -> 'StormDetail_2000.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2006_c20250122.csv' -> 'StormDetail_2006.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2011_c20230417.csv' -> 'StormDetail_2011.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2004_c20220425.csv' -> 'StormDetail_2004.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2003_c20220425.csv' -> 'StormDetail_2003.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2010_c20220425.csv' -> 'StormDetail_2010.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2008_c20240620.csv' -> 'StormDetail_2008.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2007_c20240216.csv' -> 'StormDetail_2007.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2019_c20240117.csv' -> 'StormDetail_2019.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2013_c20230118.csv' -> 'StormDetail_2013.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2009_c20231116.csv' -> 'StormDetail_2009.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2021_c20240716.csv' -> 'StormDetail_2021.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2018_c20240716.csv' -> 'StormDetail_2018.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2023_c20241216.csv' -> 'StormDetail_2023.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2012_c20221216.csv' -> 'StormDetail_2012.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2016_c20220719.csv' -> 'StormDetail_2016.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2024_c20250122.csv' -> 'StormDetail_2024.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2017_c20250122.csv' -> 'StormDetail_2017.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2002_c20220425.csv' -> 'StormDetail_2002.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2015_c20240716.csv' -> 'StormDetail_2015.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2022_c20241121.csv' -> 'StormDetail_2022.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2020_c20240620.csv' -> 'StormDetail_2020.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2005_c20220425.csv' -> 'StormDetail_2005.csv'\n",
      "Renamed 'StormEvents_details-ftp_v1.0_d2001_c20220425.csv' -> 'StormDetail_2001.csv'\n",
      "After:\n",
      "['StormDetail_2010.csv', 'StormDetail_2004.csv', 'StormDetail_2005.csv', 'StormDetail_2011.csv', 'StormDetail_2007.csv', 'StormDetail_2013.csv', 'StormDetail_2012.csv', 'StormDetail_2006.csv', '.DS_Store', 'StormDetail_2002.csv', 'StormDetail_2016.csv', 'StormDetail_2017.csv', 'StormDetail_2003.csv', 'StormDetail_2015.csv', 'StormDetail_2001.csv', 'StormDetail_2000.csv', 'StormDetail_2014.csv', 'StormDetail_2019.csv', 'StormDetail_2024.csv', 'StormDetail_2018.csv', 'StormDetail_2023.csv', 'StormDetail_2022.csv', 'StormDetail_2020.csv', 'StormDetail_2008.csv', 'StormDetail_2009.csv', 'StormDetail_2021.csv']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\")\n",
    "print(os.listdir(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 2000-2024\"))\n",
    "\n",
    "rename_storm_files(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 2000-2024\")\n",
    "\n",
    "print(\"After:\")\n",
    "print(os.listdir(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Storm Data 2000-2024\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_rename_location_data(folder_path=\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 2000-2024\"):\n",
    "    \"\"\"\n",
    "    1) Loads all 'StormEvents_locations-ftp_v1.0_dXXXX_*.csv' files from 'folder_path'.\n",
    "    2) Concatenates them into a single DataFrame.\n",
    "    3) Renames each file from 'StormEvents_locations-ftp_v1.0_dYYYY_c...csv'\n",
    "       to 'Location_YYYY.csv'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find all location CSV files that start with 'StormEvents_locations-ftp_v1.0_d'\n",
    "    pattern = os.path.join(folder_path, \"StormEvents_locations-ftp_v1.0_d*.csv\")\n",
    "    csv_files = glob.glob(pattern)\n",
    "\n",
    "    # Load them into a list of DataFrames\n",
    "    dataframes = []\n",
    "    for file_path in csv_files:\n",
    "        location = pd.read_csv(file_path)\n",
    "        dataframes.append(location)\n",
    "\n",
    "    # Rename each file based on the 4-digit year in the pattern 'dYYYY'\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        match = re.search(r\"_d(\\d{4})_\", file_name)\n",
    "        if match:\n",
    "            year = match.group(1)  # e.g., '2018'\n",
    "            new_file_name = f\"Location_{year}.csv\"\n",
    "            new_file_path = os.path.join(folder_path, new_file_name)\n",
    "            \n",
    "            # Rename on disk\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f\"Renamed '{file_name}' -> '{new_file_name}'\")\n",
    "        else:\n",
    "            print(f\"Could not find year in '{file_name}' (no rename done).\")\n",
    "\n",
    "    # Combine all DataFrames into one\n",
    "    if dataframes:\n",
    "        combined_location = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_location\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Usage example:\n",
    "location_df = load_and_rename_location_data(\"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 2000-2024\")\n",
    "print(location_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_event_location_columns(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame from your location files,\n",
    "    this function returns a new DataFrame containing only the\n",
    "    'EVENT_ID' and 'LOCATION' columns.\n",
    "    \n",
    "    If the columns are named differently (e.g., 'Location' instead of 'LOCATION'),\n",
    "    update the column names accordingly.\n",
    "    \"\"\"\n",
    "    # Select only the desired columns.\n",
    "    # Adjust the names if your actual DataFrame has a different case/spelling.\n",
    "    selected_df = df[['EVENT_ID', 'LOCATION']]\n",
    "    \n",
    "    return selected_df\n",
    "\n",
    "# Usage example:\n",
    "selected_location_df = select_event_location_columns(location_df)\n",
    "print(selected_location_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_storm_detail_location(storm_detail_folder, location_folder):\n",
    "    \"\"\"\n",
    "    For each StormDetail_XXXX.csv file in storm_detail_folder, this function:\n",
    "      1. Extracts the year (XXXX) from the filename.\n",
    "      2. Looks for a corresponding Location_XXXX.csv file in location_folder.\n",
    "      3. Loads both CSVs.\n",
    "      4. Merges them on 'EVENT_ID'.\n",
    "      5. Selects only the 'EVENT_ID' and 'location' columns.\n",
    "    \n",
    "    It returns a combined DataFrame with merged data for all years.\n",
    "    \"\"\"\n",
    "    # Pattern for storm detail files: e.g., StormDetail_2000.csv, StormDetail_2001.csv, etc.\n",
    "    storm_pattern = os.path.join(storm_detail_folder, \"StormDetail_*.csv\")\n",
    "    storm_files = glob.glob(storm_pattern)\n",
    "    \n",
    "    merged_list = []\n",
    "    \n",
    "    for storm_file in storm_files:\n",
    "        # Extract the 4-digit year from the filename\n",
    "        base_name = os.path.basename(storm_file)\n",
    "        match = re.search(r\"StormDetail_(\\d{4})\\.csv\", base_name)\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            # Construct the expected location file name for that year.\n",
    "            location_file = os.path.join(location_folder, f\"Location_{year}.csv\")\n",
    "            \n",
    "            if os.path.exists(location_file):\n",
    "                # Load the two dataframes.\n",
    "                storm_df = pd.read_csv(storm_file)\n",
    "                location_df = pd.read_csv(location_file)\n",
    "                \n",
    "                # Merge them on EVENT_ID.\n",
    "                merged_df = pd.merge(storm_df, location_df, on='EVENT_ID', how='inner')\n",
    "                \n",
    "                # Select only the EVENT_ID and location columns.\n",
    "                # (Assumes location_df already has the 'location' column; if not, \n",
    "                # you may need to rename the appropriate column first.)\n",
    "                merged_df = merged_df[['EVENT_ID', 'LOCATION']]\n",
    "                \n",
    "                merged_list.append(merged_df)\n",
    "            else:\n",
    "                print(f\"Location file for year {year} not found in {location_folder}.\")\n",
    "        else:\n",
    "            print(f\"Year not found in file name: {base_name}\")\n",
    "    \n",
    "    if merged_list:\n",
    "        # Combine all merged DataFrames into one.\n",
    "        final_merged = pd.concat(merged_list, ignore_index=True)\n",
    "        return final_merged\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "# Adjust these folder paths as needed.\n",
    "storm_detail_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/StormDetails\"\n",
    "location_folder = \"/Users/paranxiaair/Desktop/STUDY/EMORY/大三下/QTM Capstone/Deforestration-and-Flooding-in-GA/Location 2000-2024\"\n",
    "\n",
    "merged_data = merge_storm_detail_location(storm_detail_folder, location_folder)\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE CELL\n",
    "\n",
    "def filter_storm_data(df):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame so that:\n",
    "      - Only rows where STATE == 'GEORGIA' \n",
    "      - Only rows where EVENT_TYPE is in ['flash flood', 'coastal flood', 'flood']\n",
    "      - Keeps only the columns ['STATE', 'LOCATION_ID']\n",
    "    \"\"\"\n",
    "    # Filter rows\n",
    "    filtered_df = df[\n",
    "        (df[\"STATE\"] == \"GEORGIA\") &\n",
    "        (df[\"EVENT_TYPE\"].isin([\"flash flood\", \"coastal flood\", \"flood\"]))\n",
    "    ]\n",
    "    \n",
    "    # Select only the desired columns\n",
    "    filtered_df = filtered_df[[\"STATE\", \"LOCATION\", \"EVENT_TYPE\", \"EVENT_ID\"]]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Usage Example:\n",
    "filtered_storm_df = filter_storm_data(merged_data)\n",
    "print(filtered_storm_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_event_ids_by_location(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with 'LOCATION' and 'EVENT_ID' columns,\n",
    "    groups by LOCATION and counts the number of EVENT_ID occurrences.\n",
    "    \n",
    "    Returns a new DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'Event_Count'\n",
    "    \"\"\"\n",
    "    # Group by 'LOCATION' and count the occurrences of 'EVENT_ID'\n",
    "    count_df = df.groupby('LOCATION')['EVENT_ID'].count().reset_index()\n",
    "    return count_df\n",
    "\n",
    "# Usage example:\n",
    "event_counts = count_event_ids_by_location(filtered_storm_df)\n",
    "print(event_counts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_rate_change(filtered_df, year1=1985, year2=2023):\n",
    "    \"\"\"\n",
    "    Given a DataFrame that includes a 'Year' column along with \n",
    "    'LOCATION' and 'EVENT_ID' columns, this function:\n",
    "      1. Filters the data to only include rows for year1 and year2.\n",
    "      2. Counts the number of events (EVENT_ID) by LOCATION for each year.\n",
    "      3. Merges the counts (inner join on LOCATION, so only locations present in both years are kept).\n",
    "      4. Calculates the rate of change using:\n",
    "         (count_year2 - count_year1) / count_year1.\n",
    "    \n",
    "    Returns a DataFrame with columns:\n",
    "      - 'LOCATION'\n",
    "      - 'count_year1'\n",
    "      - 'count_year2'\n",
    "      - 'rate_change'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for each target year\n",
    "    df_year1 = filtered_df[filtered_df['Year'] == year1]\n",
    "    df_year2 = filtered_df[filtered_df['Year'] == year2]\n",
    "    \n",
    "    # Count events per location for each year\n",
    "    count_year1 = df_year1.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year1.rename(columns={'EVENT_ID': 'count_year1'}, inplace=True)\n",
    "    \n",
    "    count_year2 = df_year2.groupby('LOCATION', as_index=False)['EVENT_ID'].count()\n",
    "    count_year2.rename(columns={'EVENT_ID': 'count_year2'}, inplace=True)\n",
    "    \n",
    "    # Merge counts for locations present in both years\n",
    "    merged_counts = pd.merge(count_year1, count_year2, on='LOCATION', how='inner')\n",
    "    \n",
    "    # Calculate the rate of change. If count_year1 is zero, the division will produce inf/NaN.\n",
    "    merged_counts['rate_change'] = (merged_counts['count_year2'] - merged_counts['count_year1']) / merged_counts['count_year1']\n",
    "    \n",
    "    return merged_counts\n",
    "\n",
    "# Usage example:\n",
    "# Ensure that filtered\n",
    "rate_change_df = calculate_rate_change(filtered_storm_df, year1=1985, year2=2023)\n",
    "print(rate_change_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_change_df.to_csv(\"rate_change_per_location.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
